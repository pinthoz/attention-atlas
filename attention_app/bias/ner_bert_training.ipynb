{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dgdlR1m9Zw-z"
            },
            "source": [
                "# ðŸ§‘â€ðŸ«ï¸ Multi-label NER Training (Scientific Method)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "BbpPkm16-0O0"
            },
            "outputs": [],
            "source": [
                "#@title Define Hyperparameters & Configuration\n",
                "\n",
                "BATCH_SIZE = 16 #@param\n",
                "LEARNING_RATE = 5e-5 #@param\n",
                "MAX_EPOCHS = 20 #@param\n",
                "THRESHOLD = 0.5 #@param\n",
                "PATIENCE = 3 #@param {type:\"integer\"}\n",
                "\n",
                "# For focal loss function\n",
                "ALPHA = 0.65 #@param\n",
                "GAMMA = 3.5 #@param"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "lwtvYV5XAb8Z"
            },
            "outputs": [],
            "source": [
                "#@title Install Dependencies\n",
                "\n",
                "!pip install transformers pytorch-lightning torchmetrics scikit-learn seaborn matplotlib\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "A6DAGPZaYsjv"
            },
            "source": [
                "### ðŸ’¾ Pre-Processing and Data Loading\n",
                "\n",
                "We will load the dataset, merge the multi-label annotations, and perform a determinstic train/test split."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "jwEp50IIxzKf"
            },
            "outputs": [],
            "source": [
                "#@title Load & Transform Data\n",
                "import json\n",
                "import os\n",
                "import random\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Path configuration - adjust if needed\n",
                "BASE_DIR = Path('..')\n",
                "DATASET_PATH = BASE_DIR / 'dataset' / 'new_dataset.json'\n",
                "TEST_SET_OUTPUT_PATH = BASE_DIR / 'dataset' / 'held_out_test_set.json'\n",
                "\n",
                "def load_and_transform_data(path):\n",
                "    with open(path, 'r', encoding='utf-8') as f:\n",
                "        data = json.load(f)\n",
                "    \n",
                "    # The JSON has a 'bias_dataset' key\n",
                "    if 'bias_dataset' in data:\n",
                "        data = data['bias_dataset']\n",
                "    \n",
                "    transformed_data = []\n",
                "    for entry in data:\n",
                "        text_str = entry['text_str']\n",
                "        # Determine sequence length (approximate based on raw split, tokenization fixes later)\n",
                "        # The annotations are usually character or token aligned. Let's inspect the first one.\n",
                "        # 'annotations': {'GEN': [...], 'STEREO': [...], 'UNFAIR': [...]}\n",
                "        \n",
                "        annotations = entry['annotations']\n",
                "        # Verify generic length using GEN (assuming all lists are same length)\n",
                "        seq_len = len(annotations['GEN'])\n",
                "        \n",
                "        # Merge tags into list of lists\n",
                "        combined_tags = []\n",
                "        for i in range(seq_len):\n",
                "            tags_for_token = []\n",
                "            # Check each category\n",
                "            if annotations['GEN'][i] != 'O':\n",
                "                tags_for_token.append(annotations['GEN'][i])\n",
                "            if annotations['STEREO'][i] != 'O':\n",
                "                tags_for_token.append(annotations['STEREO'][i])\n",
                "            if annotations['UNFAIR'][i] != 'O':\n",
                "                tags_for_token.append(annotations['UNFAIR'][i])\n",
                "            \n",
                "            if not tags_for_token:\n",
                "                tags_for_token.append('O')\n",
                "            \n",
                "            combined_tags.append(tags_for_token)\n",
                "        \n",
                "        transformed_data.append({\n",
                "            'text_str': text_str,\n",
                "            'ner_tags': combined_tags,\n",
                "            'id': entry['id']\n",
                "        })\n",
                "    \n",
                "    return transformed_data\n",
                "\n",
                "# 1. Load Data\n",
                "if not DATASET_PATH.exists():\n",
                "    raise FileNotFoundError(f\"Dataset not found at {DATASET_PATH}\")\n",
                "\n",
                "all_data = load_and_transform_data(DATASET_PATH)\n",
                "print(f\"Loaded {len(all_data)} samples.\")\n",
                "print(\"Sample Entry 0:\", all_data[0])\n",
                "\n",
                "# 2. Deterministic Split (Train / Test)\n",
                "# Since dataset is small (200), we hold out 20% (40 samples) for pure testing.\n",
                "train_val_data, test_data = train_test_split(all_data, test_size=0.20, random_state=42, shuffle=True)\n",
                "\n",
                "print(f\"Train/Val Size: {len(train_val_data)}\")\n",
                "print(f\"Test Size: {len(test_data)}\")\n",
                "\n",
                "# Save Test Set for Reproducibility\n",
                "with open(TEST_SET_OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
                "    json.dump(test_data, f, indent=2)\n",
                "print(f\"Saved held-out test set to {TEST_SET_OUTPUT_PATH}\")\n",
                "\n",
                "ner_annotations_df = pd.DataFrame(train_val_data)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "U-lvKfdD_RmD"
            },
            "outputs": [],
            "source": [
                "#@title Tokenization Logic\n",
                "import torch\n",
                "from transformers import BertTokenizerFast\n",
                "\n",
                "# init tokenizer\n",
                "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
                "\n",
                "label2id = {\n",
                "    'O': 0,\n",
                "    'B-STEREO': 1, 'I-STEREO': 2,\n",
                "    'B-GEN': 3, 'I-GEN': 4,\n",
                "    'B-UNFAIR': 5, 'I-UNFAIR': 6\n",
                "}\n",
                "num_labels = len(label2id)\n",
                "id2label = {v: k for k, v in label2id.items()}\n",
                "\n",
                "def tokenize_and_align_labels(text, annotations, tokenizer, label2id, max_length=128):\n",
                "    # The text is already tokenized in the source? Or is it a raw string?\n",
                "    # Based on the JSON logic, 'annotations' are aligned to *original* tokens (likely whitespace split).\n",
                "    # We must assume `text.split()` aligns with `annotations` length.\n",
                "    \n",
                "    original_words = text.split()\n",
                "    if len(original_words) != len(annotations):\n",
                "        # Simple heuristic fix or skip\n",
                "        # print(f\"Warning: Length mismatch. Text: {len(original_words)}, Labels: {len(annotations)}\")\n",
                "        # Truncate to shorter\n",
                "        min_len = min(len(original_words), len(annotations))\n",
                "        original_words = original_words[:min_len]\n",
                "        annotations = annotations[:min_len]\n",
                "\n",
                "    tokenized_inputs = tokenizer(original_words, is_split_into_words=True, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
                "    word_ids = tokenized_inputs.word_ids()\n",
                "\n",
                "    aligned_labels = []\n",
                "    for word_idx in word_ids:\n",
                "        if word_idx is None:\n",
                "            aligned_labels.append([-100] * num_labels)\n",
                "        else:\n",
                "            current_tags = annotations[word_idx]\n",
                "            label_vector = [0] * num_labels\n",
                "            for tag in current_tags:\n",
                "                if tag in label2id:\n",
                "                    label_vector[label2id[tag]] = 1\n",
                "            aligned_labels.append(label_vector)\n",
                "            \n",
                "    return tokenized_inputs, aligned_labels\n",
                "\n",
                "def preprocess_data(df, tokenizer, label2id, max_length=128):\n",
                "    tokenized_inputs_list = []\n",
                "    aligned_labels_list = []\n",
                "    for _, row in df.iterrows():\n",
                "        text = row['text_str']\n",
                "        annotations = row['ner_tags']\n",
                "        inputs, labels = tokenize_and_align_labels(text, annotations, tokenizer, label2id, max_length)\n",
                "        tokenized_inputs_list.append(inputs)\n",
                "        aligned_labels_list.append(labels)\n",
                "    return tokenized_inputs_list, aligned_labels_list\n",
                "\n",
                "# Preprocess Train/Val Data\n",
                "tokenized_texts, labels = preprocess_data(ner_annotations_df, tokenizer, label2id)\n",
                "print(f\"Processed {len(tokenized_texts)} training samples.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "G2XS1I7ZAC9Y"
            },
            "outputs": [],
            "source": [
                "#@title Lightning Data Module\n",
                "from torch.utils.data import Dataset, DataLoader, random_split\n",
                "import pytorch_lightning as pl\n",
                "\n",
                "class NERDataset(Dataset):\n",
                "    def __init__(self, tokenized_texts, labels):\n",
                "        self.input_ids = [t['input_ids'].squeeze() for t in tokenized_texts]\n",
                "        self.attention_mask = [t['attention_mask'].squeeze() for t in tokenized_texts]\n",
                "        self.labels = labels\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.input_ids)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return self.input_ids[idx], self.attention_mask[idx], torch.tensor(self.labels[idx])\n",
                "\n",
                "class NERDataModule(pl.LightningDataModule):\n",
                "    def __init__(self, tokenized_texts, labels, batch_size=16, val_split=0.15):\n",
                "        super().__init__()\n",
                "        self.dataset = NERDataset(tokenized_texts, labels)\n",
                "        self.batch_size = batch_size\n",
                "        self.val_split = val_split\n",
                "\n",
                "    def setup(self, stage=None):\n",
                "        val_size = int(len(self.dataset) * self.val_split)\n",
                "        train_size = len(self.dataset) - val_size\n",
                "        self.train_ds, self.val_ds = random_split(self.dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
                "\n",
                "    def train_dataloader(self):\n",
                "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True, num_workers=0)\n",
                "\n",
                "    def val_dataloader(self):\n",
                "        return DataLoader(self.val_ds, batch_size=self.batch_size, num_workers=0)\n",
                "\n",
                "data_module = NERDataModule(tokenized_texts, labels, batch_size=BATCH_SIZE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "VV1EVlZU5a6z"
            },
            "source": [
                "### ðŸ§  Model Definition with Early Stopping"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "vi6Xyts3v6LW"
            },
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "from torch.optim import AdamW\n",
                "from transformers import BertForTokenClassification, get_linear_schedule_with_warmup\n",
                "from pytorch_lightning import LightningModule\n",
                "from torchmetrics.classification import MultilabelPrecision, MultilabelRecall, MultilabelF1Score\n",
                "from torchmetrics import HammingDistance\n",
                "\n",
                "class NERModel(LightningModule):\n",
                "    def __init__(self, learning_rate=5e-5, threshold=0.5, alpha=0.75, gamma=3):\n",
                "        super().__init__()\n",
                "        self.save_hyperparameters()\n",
                "        self.bert = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=7)\n",
                "        self.learning_rate = learning_rate\n",
                "        self.threshold = threshold\n",
                "        self.alpha = alpha \n",
                "        self.gamma = gamma\n",
                "        \n",
                "        # Metrics (ignoring 'O' class usually, but keeping simple here)\n",
                "        self.f1 = MultilabelF1Score(num_labels=4, average='macro', threshold=0.5)\n",
                "        self.hamming = HammingDistance(task='multilabel', num_labels=4, threshold=0.5)\n",
                "\n",
                "    def forward(self, input_ids, attention_mask):\n",
                "        return self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
                "\n",
                "    def focal_loss(self, logits, labels):\n",
                "        # Flatten: (batch * seq_len, num_labels)\n",
                "        logits = logits.view(-1, 7)\n",
                "        labels = labels.view(-1, 7).float()\n",
                "        \n",
                "        # Mask ignored tokens (-100)\n",
                "        mask = (labels[:, 0] != -100) # Assuming if first dim is -100 whole vec is ignored. Or verify mask logic.\n",
                "        # Actually, in our vector encoding, -100 is likely passed as [-100, -100...]. \n",
                "        # Let's check a sum condition or similar. \n",
                "        # Safe check: valid vectors have 0 or 1. Invalid have -100.\n",
                "        valid_mask = (labels >= 0).all(dim=1)\n",
                "        \n",
                "        logits = logits[valid_mask]\n",
                "        labels = labels[valid_mask]\n",
                "        \n",
                "        if logits.shape[0] == 0:\n",
                "             return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
                "\n",
                "        bce_loss = F.binary_cross_entropy_with_logits(logits, labels, reduction='none')\n",
                "        p_t = torch.exp(-bce_loss)\n",
                "        loss = self.alpha * (1 - p_t) ** self.gamma * bce_loss\n",
                "        return loss.mean()\n",
                "\n",
                "    def training_step(self, batch, batch_idx):\n",
                "        outputs = self(batch[0], batch[1])\n",
                "        loss = self.focal_loss(outputs, batch[2])\n",
                "        self.log('train_loss', loss, prog_bar=True)\n",
                "        return loss\n",
                "\n",
                "    def validation_step(self, batch, batch_idx):\n",
                "        outputs = self(batch[0], batch[1])\n",
                "        loss = self.focal_loss(outputs, batch[2])\n",
                "        self.log('val_loss', loss, prog_bar=True)\n",
                "        return loss\n",
                "\n",
                "    def configure_optimizers(self):\n",
                "        optimizer = AdamW(self.parameters(), lr=self.learning_rate)\n",
                "        scheduler = get_linear_schedule_with_warmup(\n",
                "            optimizer, \n",
                "            num_warmup_steps=int(0.1 * self.trainer.estimated_stepping_batches),\n",
                "            num_training_steps=self.trainer.estimated_stepping_batches\n",
                "        )\n",
                "        return [optimizer], [scheduler]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "dnn6FfmqyAIe"
            },
            "outputs": [],
            "source": [
                "#@title Train with Early Stopping\n",
                "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
                "from pytorch_lightning.loggers import TensorBoardLogger\n",
                "import shutil\n",
                "\n",
                "# Clean logs\n",
                "shutil.rmtree('lightning_logs', ignore_errors=True)\n",
                "\n",
                "checkpoint_callback = ModelCheckpoint(\n",
                "    monitor='val_loss',\n",
                "    dirpath='checkpoints',\n",
                "    filename='gusnet-best-{epoch:02d}-{val_loss:.2f}',\n",
                "    save_top_k=1,\n",
                "    mode='min',\n",
                "    save_last=True\n",
                ")\n",
                "\n",
                "early_stop_callback = EarlyStopping(\n",
                "    monitor='val_loss',\n",
                "    patience=PATIENCE,\n",
                "    verbose=True,\n",
                "    mode='min'\n",
                ")\n",
                "\n",
                "model = NERModel()\n",
                "trainer = pl.Trainer(\n",
                "    max_epochs=MAX_EPOCHS,\n",
                "    callbacks=[checkpoint_callback, early_stop_callback],\n",
                "    accelerator='auto',\n",
                "    devices=1,\n",
                "    enable_progress_bar=True\n",
                ")\n",
                "\n",
                "trainer.fit(model, data_module)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Evaluate"
            },
            "outputs": [],
            "source": [
                "#@title Evaluate on Held-Out Test Set\n",
                "\n",
                "# Load best model\n",
                "best_model_path = checkpoint_callback.best_model_path\n",
                "print(f\"Loading best model from {best_model_path}\")\n",
                "best_model = NERModel.load_from_checkpoint(best_model_path)\n",
                "best_model.eval()\n",
                "best_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "# Prepare Test Data\n",
                "test_inputs = []\n",
                "test_labels = []\n",
                "test_df = pd.DataFrame(test_data)\n",
                "\n",
                "test_in, test_lbl = preprocess_data(test_df, tokenizer, label2id)\n",
                "test_ds = NERDataset(test_in, test_lbl)\n",
                "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
                "\n",
                "# Inference Loop\n",
                "from sklearn.metrics import classification_report, accuracy_score\n",
                "\n",
                "all_preds = []\n",
                "all_trues = []\n",
                "\n",
                "device = best_model.device\n",
                "with torch.no_grad():\n",
                "    for batch in test_dl:\n",
                "        input_ids = batch[0].to(device)\n",
                "        mask = batch[1].to(device)\n",
                "        labels = batch[2].cpu().numpy()\n",
                "        \n",
                "        logits = best_model(input_ids, mask)\n",
                "        probs = torch.sigmoid(logits).cpu().numpy()\n",
                "        preds = (probs > THRESHOLD).astype(int)\n",
                "        \n",
                "        # Flatten and filter padded -100\n",
                "        for i in range(len(labels)):\n",
                "            valid_indices = np.where((labels[i] >= 0).all(axis=1))[0]\n",
                "            if len(valid_indices) > 0:\n",
                "                p = preds[i][valid_indices]\n",
                "                l = labels[i][valid_indices]\n",
                "                all_preds.extend(p)\n",
                "                all_trues.extend(l)\n",
                "\n",
                "all_preds = np.array(all_preds)\n",
                "all_trues = np.array(all_trues)\n",
                "\n",
                "print(\"\\n--- Test Set Results ---\")\n",
                "# Calculate per-class metrics\n",
                "target_names = ['O', 'B-STEREO', 'I-STEREO', 'B-GEN', 'I-GEN', 'B-UNFAIR', 'I-UNFAIR']\n",
                "print(classification_report(all_trues, all_preds, target_names=target_names, zero_division=0))\n",
                "\n",
                "# Entity Level Custom Metrics (Hamming etc)\n",
                "print(f\"Exact Match Pct: {accuracy_score(all_trues, all_preds):.4f}\")\n"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}