{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUS-Net with GPT-2: Social Bias Token Classification\n",
    "\n",
    "This notebook trains a **GPT-2 based** multi-label token classifier for social bias detection,\n",
    "replicating the GUS-Net methodology on the same dataset but swapping the BERT backbone for GPT-2.\n",
    "\n",
    "**Key differences from the BERT version:**\n",
    "- GPT-2 uses **BPE** tokenization (vs BERT's WordPiece)\n",
    "- GPT-2 is **autoregressive** (left-to-right causal attention) — each token only sees left context\n",
    "- No `[CLS]`/`[SEP]` tokens — GPT-2 uses `<|endoftext|>`\n",
    "- LLRD targets `transformer.h.{i}` layers (vs `bert.encoder.layer.{i}`)\n",
    "\n",
    "**Training pipeline** (identical to BERT version):\n",
    "- Multi-label sigmoid with 6 channels (B/I × GEN/UNFAIR/STEREO)\n",
    "- Focal Loss with per-channel α weighting\n",
    "- Layer-wise Learning Rate Decay (LLRD)\n",
    "- Cosine annealing + warmup\n",
    "- SWA (Stochastic Weight Averaging)\n",
    "- Two-pass threshold optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5b38838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available:  True\n",
      "GPU: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast,\n",
    "    GPT2ForTokenClassification,\n",
    "    AutoConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available:  {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e52a02",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "34b18b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 3739 examples\n",
      "Columns: ['text_str', 'ner_tags', 'rationale', 'individual_ner_annotations_json', 'id']\n",
      "\n",
      "Sample entry:\n",
      "{\n",
      "  \"text_str\": \"Should we assume that every retired individual spends hours playing bingo each week?\",\n",
      "  \"ner_tags\": \"[['O'], ['O'], ['O'], ['O'], ['B-GEN'], ['I-GEN', 'B-STEREO'], ['I-GEN', 'I-STEREO'], ['I-STEREO'], ['I-STEREO'], ['I-STEREO'], ['I-STEREO'], ['I-STEREO'], ['I-STEREO']]\",\n",
      "  \"rationale\": \"['Reasoning: Let\\\\'s think step by step in order to produce the annotations. We will analyze ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"ethical-spectacle/gus-dataset-v1\", split=\"train\")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} examples\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "print(\"\\nSample entry:\")\n",
    "print(json.dumps(dataset[0], indent=2, ensure_ascii=False)[:400] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38b51f",
   "metadata": {},
   "source": [
    "## 3. Tokenization with GPT-2 BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "966452d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:    50257\n",
      "Pad token:     '<|endoftext|>' (id=50256)\n",
      "Padding side:  right\n"
     ]
    }
   ],
   "source": [
    "# ── GPT-2 tokenizer setup ──\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", add_prefix_space=True)\n",
    "\n",
    "# GPT-2 has no pad token by default — use eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Pad on the RIGHT (like BERT) so label indices align correctly\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Vocab size:    {tokenizer.vocab_size}\")\n",
    "print(f\"Pad token:     {tokenizer.pad_token!r} (id={tokenizer.pad_token_id})\")\n",
    "print(f\"Padding side:  {tokenizer.padding_side}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1aa6d7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels (6): ['B-GEN', 'I-GEN', 'B-UNFAIR', 'I-UNFAIR', 'B-STEREO', 'I-STEREO']\n"
     ]
    }
   ],
   "source": [
    "# ── Multi-label channel definition ──\n",
    "# 6 binary channels (no explicit O — O = all zeros)\n",
    "channels = [\"B-GEN\", \"I-GEN\", \"B-UNFAIR\", \"I-UNFAIR\", \"B-STEREO\", \"I-STEREO\"]\n",
    "channel2idx = {c: i for i, c in enumerate(channels)}\n",
    "idx2channel = {i: c for i, c in enumerate(channels)}\n",
    "num_channels = len(channels)\n",
    "\n",
    "print(f\"Channels ({num_channels}): {channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a3196046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(example):\n",
    "    \"\"\"Extract per-word multi-hot tags from the ner_tags string.\"\"\"\n",
    "    # The HF dataset provides 'ner_tags' as a string representation of a list of lists.\n",
    "    # e.g. \"[['O'], ['B-GEN'], ...]\"\n",
    "    return ast.literal_eval(example[\"ner_tags\"])\n",
    "\n",
    "def prepare_example(example):\n",
    "    \"\"\"Tokenize with GPT-2 BPE and build multi-hot label matrix.\n",
    "\n",
    "    Steps:\n",
    "      1. Split text into words\n",
    "      2. Tokenize with is_split_into_words=True (preserves word→subword map)\n",
    "      3. Use word_ids() to align BPE subwords to original words\n",
    "      4. Build [seq_len, num_channels] multi-hot labels\n",
    "      5. Mask special/padding positions with -100\n",
    "    \"\"\"\n",
    "    text = example[\"text_str\"]\n",
    "    word_tags = parse_annotations(example)\n",
    "    words = text.split()\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        words,\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    word_ids = tokenized.word_ids()\n",
    "    seq_len = len(word_ids)\n",
    "    labels_multi = np.zeros((seq_len, num_channels), dtype=np.float32)\n",
    "\n",
    "    prev_word_id = None\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            prev_word_id = None\n",
    "            continue\n",
    "        if word_id >= len(word_tags):\n",
    "            prev_word_id = word_id\n",
    "            continue\n",
    "        tags = word_tags[word_id]\n",
    "        for tag in tags:\n",
    "            if tag == \"O\":\n",
    "                continue\n",
    "            # Continuation subword: B- → I-\n",
    "            if word_id == prev_word_id:\n",
    "                if tag.startswith(\"B-\"):\n",
    "                    i_tag = \"I-\" + tag[2:]\n",
    "                    if i_tag in channel2idx:\n",
    "                        labels_multi[idx, channel2idx[i_tag]] = 1.0\n",
    "                elif tag in channel2idx:\n",
    "                    labels_multi[idx, channel2idx[tag]] = 1.0\n",
    "            else:\n",
    "                if tag in channel2idx:\n",
    "                    labels_multi[idx, channel2idx[tag]] = 1.0\n",
    "        prev_word_id = word_id\n",
    "\n",
    "    # Mask padding / special positions with -100\n",
    "    final_labels = []\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            final_labels.append([-100.0] * num_channels)\n",
    "        else:\n",
    "            final_labels.append(labels_multi[idx].tolist())\n",
    "\n",
    "    tokenized[\"labels\"] = final_labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "97019f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset with GPT-2 BPE...\n",
      "Tokenization complete!\n",
      "\n",
      "Sanity check:\n",
      "  Total valid tokens:  67503\n",
      "  Tokens with bias:    21450\n",
      "  Positive rate:       31.78%\n",
      "  OK\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing dataset with GPT-2 BPE...\")\n",
    "tokenized_dataset = DatasetDict({\n",
    "    \"train\": dataset.map(\n",
    "        prepare_example,\n",
    "        batched=False,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "})\n",
    "print(\"Tokenization complete!\")\n",
    "\n",
    "# ── Sanity check ──\n",
    "total_positive = 0\n",
    "total_valid = 0\n",
    "for ex in tokenized_dataset[\"train\"]:\n",
    "    labels = np.array(ex[\"labels\"])\n",
    "    valid = labels[labels[:, 0] != -100.0]\n",
    "    total_valid += len(valid)\n",
    "    total_positive += (valid > 0).any(axis=1).sum()\n",
    "\n",
    "print(f\"\\nSanity check:\")\n",
    "print(f\"  Total valid tokens:  {total_valid}\")\n",
    "print(f\"  Tokens with bias:    {total_positive}\")\n",
    "print(f\"  Positive rate:       {total_positive / max(total_valid, 1):.2%}\")\n",
    "assert total_positive > 0, \"FATAL: No positive labels found.\"\n",
    "print(\"  OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58f73b",
   "metadata": {},
   "source": [
    "## 4. Train / Dev / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a454c0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2617\n",
      "Dev:   561\n",
      "Test:  561\n"
     ]
    }
   ],
   "source": [
    "# 70% train, 15% dev, 15% test\n",
    "train_devtest = tokenized_dataset[\"train\"].train_test_split(test_size=0.30, seed=42)\n",
    "train_split = train_devtest[\"train\"]\n",
    "dev_test = train_devtest[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "dev_split = dev_test[\"train\"]\n",
    "test_split = dev_test[\"test\"]\n",
    "\n",
    "print(f\"Train: {len(train_split)}\")\n",
    "print(f\"Dev:   {len(dev_split)}\")\n",
    "print(f\"Test:  {len(test_split)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1ce0d",
   "metadata": {},
   "source": [
    "## 5. GPT-2 Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7b882e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:       gpt2\n",
      "Parameters:  124,444,422\n",
      "Layers:      12\n",
      "Heads:       12\n",
      "Hidden dim:  768\n",
      "Classifier dropout: 0.3\n",
      "Residual dropout:   0.15\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "config.num_labels = num_channels\n",
    "config.problem_type = \"multi_label_classification\"\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "config.classifier_dropout = 0.3\n",
    "config.resid_pdrop = 0.15\n",
    "config.embd_pdrop = 0.15\n",
    "config.attn_pdrop = 0.15\n",
    "\n",
    "model = GPT2ForTokenClassification.from_pretrained(\"gpt2\", config=config)\n",
    "# Resize embeddings to account for pad token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"Model:       {config.model_type}\")\n",
    "print(f\"Parameters:  {model.num_parameters():,}\")\n",
    "print(f\"Layers:      {config.n_layer}\")\n",
    "print(f\"Heads:       {config.n_head}\")\n",
    "print(f\"Hidden dim:  {config.n_embd}\")\n",
    "print(f\"Classifier dropout: {config.classifier_dropout}\")\n",
    "print(f\"Residual dropout:   {config.resid_pdrop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe1bd4a",
   "metadata": {},
   "source": [
    "### Optional: Disable Causal Mask (Bidirectional Mode)\n",
    "\n",
    "GPT-2 uses a **causal (triangular) attention mask** by default — each token only sees\n",
    "tokens to its left.  For NER this is a disadvantage because right context helps\n",
    "disambiguate tokens.\n",
    "\n",
    "Uncommenting the cell below replaces the causal mask with a full mask, making GPT-2\n",
    "effectively bidirectional.  This breaks the pre-training assumption but can improve NER\n",
    "performance.  Keep it **commented** for a fair causal-vs-bidirectional comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93d81f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Uncomment to enable bidirectional attention ──\n",
    "# for block in model.transformer.h:\n",
    "#     block.attn.bias.fill_(True)\n",
    "# print(\"Causal mask DISABLED — GPT-2 is now bidirectional.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee258cac",
   "metadata": {},
   "source": [
    "## 6. Focal Loss & Channel Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e62a28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel statistics:\n",
      "  B-GEN     :   3532 positives, α=0.0830\n",
      "  I-GEN     :   3332 positives, α=0.0880\n",
      "  B-UNFAIR  :    753 positives, α=0.3893\n",
      "  I-UNFAIR  :   1859 positives, α=0.1577\n",
      "  B-STEREO  :   1177 positives, α=0.2491\n",
      "  I-STEREO  :   8908 positives, α=0.0329\n",
      "\n",
      "Total valid tokens: 47182\n"
     ]
    }
   ],
   "source": [
    "def estimate_channel_frequencies(dataset_split):\n",
    "    \"\"\"Estimate positive-label frequency per channel.\"\"\"\n",
    "    positives = np.zeros(num_channels, dtype=np.int64)\n",
    "    total = 0\n",
    "    for ex in dataset_split:\n",
    "        labels = np.array(ex[\"labels\"])\n",
    "        valid = labels[labels[:, 0] != -100.0]\n",
    "        if valid.size == 0:\n",
    "            continue\n",
    "        positives += valid.sum(axis=0).astype(np.int64)\n",
    "        total += valid.shape[0]\n",
    "    return positives, total\n",
    "\n",
    "\n",
    "channel_pos, total_tokens = estimate_channel_frequencies(train_split)\n",
    "channel_pos = np.maximum(channel_pos, 1)\n",
    "freq = channel_pos / float(total_tokens)\n",
    "inv_freq = 1.0 / freq\n",
    "alpha_channel = inv_freq / inv_freq.sum()\n",
    "alpha_channel = torch.tensor(alpha_channel, dtype=torch.float32)\n",
    "\n",
    "print(\"Channel statistics:\")\n",
    "for i, ch in enumerate(channels):\n",
    "    print(f\"  {ch:10s}: {channel_pos[i]:>6} positives, α={alpha_channel[i]:.4f}\")\n",
    "print(f\"\\nTotal valid tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "38daf542",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossMultiLabel(nn.Module):\n",
    "    \"\"\"Channel-wise focal loss for multi-label token classification.\n",
    "\n",
    "    L = α_c · (1 − p_t)^γ · BCE(logit, target)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha, gamma=2.0, reduction=\"mean\", label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"alpha\", alpha)\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.label_smoothing > 0:\n",
    "            targets = targets * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "        bce = F.binary_cross_entropy_with_logits(inputs, targets.float(), reduction=\"none\")\n",
    "        pt = torch.exp(-bce)\n",
    "        focal = self.alpha.to(inputs.device) * (1 - pt) ** self.gamma * bce\n",
    "        return focal.mean() if self.reduction == \"mean\" else focal.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116ed9c",
   "metadata": {},
   "source": [
    "## 7. Trainer with GPT-2 LLRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bcd168fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossTrainerGPT2(Trainer):\n",
    "    \"\"\"Trainer with Focal Loss and Layer-wise LR Decay for GPT-2.\n",
    "\n",
    "    GPT-2 parameter groups:\n",
    "      - classifier.*                         → classifier_lr\n",
    "      - transformer.h.{11..0}.*              → base_lr × decay^(11−i)\n",
    "      - transformer.wte, wpe, ln_f           → base_lr × decay^12  (embeddings)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, alpha_channel, gamma=2.0, label_smoothing=0.0,\n",
    "                 llrd_decay_factor=0.85, classifier_lr=2e-4, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.focal_loss = FocalLossMultiLabel(\n",
    "            alpha=alpha_channel, gamma=gamma, label_smoothing=label_smoothing,\n",
    "        )\n",
    "        self.llrd_decay_factor = llrd_decay_factor\n",
    "        self.classifier_lr = classifier_lr\n",
    "\n",
    "    # ── LLRD optimizer ──────────────────────────────────────────────\n",
    "    def create_optimizer(self):\n",
    "        base_lr = self.args.learning_rate\n",
    "        decay = self.llrd_decay_factor\n",
    "        # GPT-2 layer norms use ln_1 / ln_2 / ln_f (not LayerNorm)\n",
    "        no_decay_keys = [\"bias\", \"ln_1.weight\", \"ln_2.weight\", \"ln_f.weight\"]\n",
    "        n_layers = self.model.config.n_layer  # 12 for gpt2-base\n",
    "\n",
    "        opt_params = []\n",
    "\n",
    "        # 1) Classifier head — highest LR\n",
    "        opt_params.append({\n",
    "            \"params\": [p for n, p in self.model.named_parameters()\n",
    "                        if \"classifier\" in n or \"score\" in n],\n",
    "            \"lr\": self.classifier_lr,\n",
    "            \"weight_decay\": 0.0,\n",
    "        })\n",
    "\n",
    "        # 2) Transformer blocks: h.{n_layers-1} → h.0\n",
    "        for layer_idx in range(n_layers - 1, -1, -1):\n",
    "            layer_lr = base_lr * (decay ** (n_layers - 1 - layer_idx))\n",
    "            layer_prefix = f\"transformer.h.{layer_idx}.\"\n",
    "            d, nd = [], []\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if layer_prefix in n:\n",
    "                    (nd if any(k in n for k in no_decay_keys) else d).append(p)\n",
    "            if d:\n",
    "                opt_params.append({\"params\": d, \"lr\": layer_lr,\n",
    "                                   \"weight_decay\": self.args.weight_decay})\n",
    "            if nd:\n",
    "                opt_params.append({\"params\": nd, \"lr\": layer_lr,\n",
    "                                   \"weight_decay\": 0.0})\n",
    "\n",
    "        # 3) Embeddings + final LN — lowest LR\n",
    "        emb_lr = base_lr * (decay ** n_layers)\n",
    "        emb_names = [\"transformer.wte\", \"transformer.wpe\", \"transformer.ln_f\"]\n",
    "        d, nd = [], []\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if any(n.startswith(prefix) for prefix in emb_names):\n",
    "                (nd if any(k in n for k in no_decay_keys) else d).append(p)\n",
    "        if d:\n",
    "            opt_params.append({\"params\": d, \"lr\": emb_lr,\n",
    "                               \"weight_decay\": self.args.weight_decay})\n",
    "        if nd:\n",
    "            opt_params.append({\"params\": nd, \"lr\": emb_lr, \"weight_decay\": 0.0})\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(opt_params, lr=base_lr, eps=1e-8)\n",
    "        print(f\"LLRD optimizer (GPT-2):\")\n",
    "        print(f\"  Classifier LR:     {self.classifier_lr}\")\n",
    "        print(f\"  Top layer LR:      {base_lr}\")\n",
    "        print(f\"  Bottom layer LR:   {base_lr * decay**(n_layers-1):.2e}\")\n",
    "        print(f\"  Embeddings LR:     {emb_lr:.2e}\")\n",
    "        return self.optimizer\n",
    "\n",
    "    # ── Cosine scheduler ────────────────────────────────────────────\n",
    "    def create_scheduler(self, num_training_steps, optimizer=None):\n",
    "        if optimizer is None:\n",
    "            optimizer = self.optimizer\n",
    "        warmup = int(num_training_steps * self.args.warmup_ratio)\n",
    "        self.lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=warmup,\n",
    "            num_training_steps=num_training_steps,\n",
    "        )\n",
    "        print(f\"Cosine scheduler: {warmup} warmup / {num_training_steps} total\")\n",
    "        return self.lr_scheduler\n",
    "\n",
    "    # ── Loss ────────────────────────────────────────────────────────\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        logits_flat = logits.view(-1, num_channels)\n",
    "        labels_flat = labels.view(-1, num_channels)\n",
    "        valid = labels_flat[:, 0] != -100.0\n",
    "        loss = self.focal_loss(logits_flat[valid], labels_flat[valid])\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8a4523d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Metrics ──\n",
    "thresholds = np.array([0.5] * num_channels, dtype=np.float32)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-predictions))\n",
    "    valid = labels[:, :, 0] != -100.0\n",
    "    probs_flat = probs[valid]\n",
    "    labels_flat = labels[valid]\n",
    "    thr = thresholds.reshape(1, num_channels)\n",
    "    preds_bin = (probs_flat >= thr).astype(int)\n",
    "    labels_bin = labels_flat.astype(int)\n",
    "\n",
    "    if labels_bin.sum() == 0:\n",
    "        return {\"f1_macro\": 0.0, \"precision_macro\": 0.0,\n",
    "                \"recall_macro\": 0.0, \"hamming_loss\": 0.0}\n",
    "\n",
    "    channel_f1s = [f1_score(labels_bin[:, c], preds_bin[:, c],\n",
    "                            average=\"binary\", zero_division=0)\n",
    "                   for c in range(num_channels)]\n",
    "    return {\n",
    "        \"f1_macro\": np.mean(channel_f1s),\n",
    "        \"precision_macro\": precision_score(labels_bin, preds_bin,\n",
    "                                           average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(labels_bin, preds_bin,\n",
    "                                      average=\"macro\", zero_division=0),\n",
    "        \"hamming_loss\": np.mean(preds_bin != labels_bin),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c164f756",
   "metadata": {},
   "source": [
    "## 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5f777e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready:\n",
      "  Focal Loss (γ=2.0, smoothing=0.05)\n",
      "  LLRD (decay=0.85, classifier_lr=2e-4)\n",
      "  Cosine scheduler + early stopping (patience=5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:645: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gus-net-gpt2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,    # effective batch = 32\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = FocalLossTrainerGPT2(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_split,\n",
    "    eval_dataset=dev_split,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    alpha_channel=alpha_channel,\n",
    "    gamma=2.0,\n",
    "    label_smoothing=0.05,\n",
    "    llrd_decay_factor=0.85,\n",
    "    classifier_lr=2e-4,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")\n",
    "\n",
    "print(\"Trainer ready:\")\n",
    "print(f\"  Focal Loss (γ=2.0, smoothing=0.05)\")\n",
    "print(f\"  LLRD (decay=0.85, classifier_lr=2e-4)\")\n",
    "print(f\"  Cosine scheduler + early stopping (patience=5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f41491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "LLRD optimizer (GPT-2):\n",
      "  Classifier LR:     0.0002\n",
      "  Top layer LR:      5e-05\n",
      "  Bottom layer LR:   8.37e-06\n",
      "  Embeddings LR:     7.11e-06\n",
      "Cosine scheduler: 164 warmup / 1640 total\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53b7092af0a444fa84499763e491a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"\\nTraining loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Runtime:       {train_result.metrics['train_runtime']:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee48d4",
   "metadata": {},
   "source": [
    "## 9. Stochastic Weight Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a88f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in &lt;module&gt;:53                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">50 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"SWA: no valid checkpoints found.\"</span>)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">51 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">52 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>53 apply_swa(<span style=\"font-weight: bold; text-decoration: underline\">trainer</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"./gus-net-gpt2\"</span>, last_n=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">54 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'trainer'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in <module>:53                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m50 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mSWA: no valid checkpoints found.\u001b[0m\u001b[33m\"\u001b[0m)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m51 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m52 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m53 apply_swa(\u001b[1;4mtrainer\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33m./gus-net-gpt2\u001b[0m\u001b[33m\"\u001b[0m, last_n=\u001b[94m5\u001b[0m)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m54 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'trainer'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def apply_swa(trainer, checkpoint_dir, last_n=5):\n",
    "    \"\"\"Average weights from the last N checkpoints (memory-safe).\"\"\"\n",
    "    checkpoints = sorted(\n",
    "        glob.glob(f\"{checkpoint_dir}/checkpoint-*\"),\n",
    "        key=lambda x: int(x.split(\"-\")[-1]),\n",
    "    )\n",
    "    if len(checkpoints) < 2:\n",
    "        print(f\"Only {len(checkpoints)} checkpoint(s), skipping SWA.\")\n",
    "        return\n",
    "\n",
    "    last = checkpoints[-last_n:]\n",
    "    print(f\"SWA: averaging {len(last)} checkpoints\")\n",
    "\n",
    "    # Free memory before loading checkpoints\n",
    "    trainer.model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # Running average to avoid holding all states at once\n",
    "    avg = None\n",
    "    n = 0\n",
    "    for cp in last:\n",
    "        sf = os.path.join(cp, \"model.safetensors\")\n",
    "        bf = os.path.join(cp, \"pytorch_model.bin\")\n",
    "        if os.path.exists(sf):\n",
    "            from safetensors.torch import load_file\n",
    "            state = load_file(sf, device=\"cpu\")\n",
    "        elif os.path.exists(bf):\n",
    "            state = torch.load(bf, map_location=\"cpu\", weights_only=True)\n",
    "        else:\n",
    "            continue\n",
    "        n += 1\n",
    "        if avg is None:\n",
    "            avg = {k: v.float() for k, v in state.items()}\n",
    "        else:\n",
    "            # Running average: avg = avg + (new - avg) / n\n",
    "            for k in avg:\n",
    "                avg[k] += (state[k].float() - avg[k]) / n\n",
    "        del state\n",
    "        gc.collect()\n",
    "\n",
    "    if avg and n > 0:\n",
    "        trainer.model.load_state_dict(avg)\n",
    "        del avg\n",
    "        gc.collect()\n",
    "        trainer.model.to(trainer.args.device)\n",
    "        print(f\"SWA applied ({n} checkpoints).\")\n",
    "    else:\n",
    "        trainer.model.to(trainer.args.device)\n",
    "        print(\"SWA: no valid checkpoints found.\")\n",
    "\n",
    "\n",
    "apply_swa(trainer, \"./gus-net-gpt2\", last_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f0355c",
   "metadata": {},
   "source": [
    "## 10. Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc17e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in &lt;module&gt;:58                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> best_thr, macro                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">56 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">57 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>58 best_thr, best_f1_dev = optimize_thresholds(<span style=\"font-weight: bold; text-decoration: underline\">trainer</span>, dev_split)                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">59 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"\\nOptimized thresholds: {</span>best_thr<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">60 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Dev macro-F1: {</span>best_f1_dev<span style=\"color: #808000; text-decoration-color: #808000\">:.4f}\"</span>)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">61 </span>thresholds = best_thr                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'trainer'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in <module>:58                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m55 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m best_thr, macro                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m56 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m57 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m58 best_thr, best_f1_dev = optimize_thresholds(\u001b[1;4mtrainer\u001b[0m, dev_split)                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m59 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33mOptimized thresholds: \u001b[0m\u001b[33m{\u001b[0mbest_thr\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m60 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mDev macro-F1: \u001b[0m\u001b[33m{\u001b[0mbest_f1_dev\u001b[33m:\u001b[0m\u001b[33m.4f\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m61 \u001b[0mthresholds = best_thr                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'trainer'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def optimize_thresholds(trainer, dev_dataset):\n",
    "    \"\"\"Two-pass threshold optimization: coarse grid + scipy refinement.\"\"\"\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "    grid = np.arange(0.05, 0.96, 0.025).tolist()\n",
    "\n",
    "    all_probs, all_labels = [], []\n",
    "    for batch in trainer.get_eval_dataloader(dev_dataset):\n",
    "        with torch.no_grad():\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n",
    "            logits = model(**inputs).logits.cpu().numpy()\n",
    "        all_probs.append(1 / (1 + np.exp(-logits)))\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    probs = np.concatenate(all_probs)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    valid = labels[:, :, 0] != -100.0\n",
    "    pf = probs[valid]\n",
    "    lf = labels[valid].astype(int)\n",
    "\n",
    "    best_thr = np.zeros(num_channels, dtype=np.float32)\n",
    "\n",
    "    # Pass 1: coarse\n",
    "    print(\"Pass 1 — grid search:\")\n",
    "    for c in range(num_channels):\n",
    "        best_f1, best_t = 0, 0.5\n",
    "        for t in grid:\n",
    "            f1 = f1_score(lf[:, c], (pf[:, c] >= t).astype(int),\n",
    "                          average=\"binary\", zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_t = f1, t\n",
    "        best_thr[c] = best_t\n",
    "        print(f\"  {channels[c]:10s}: thr={best_t:.3f}, F1={best_f1:.4f}\")\n",
    "\n",
    "    # Pass 2: scipy\n",
    "    print(\"\\nPass 2 — refinement:\")\n",
    "    for c in range(num_channels):\n",
    "        lo = max(0.01, best_thr[c] - 0.05)\n",
    "        hi = min(0.99, best_thr[c] + 0.05)\n",
    "        res = minimize_scalar(\n",
    "            lambda t: -f1_score(lf[:, c], (pf[:, c] >= t).astype(int),\n",
    "                                average=\"binary\", zero_division=0),\n",
    "            bounds=(lo, hi), method=\"bounded\",\n",
    "        )\n",
    "        if -res.fun >= f1_score(lf[:, c], (pf[:, c] >= best_thr[c]).astype(int),\n",
    "                                 average=\"binary\", zero_division=0):\n",
    "            best_thr[c] = res.x\n",
    "        print(f\"  {channels[c]:10s}: thr={best_thr[c]:.4f}, F1={-res.fun:.4f}\")\n",
    "\n",
    "    # Global macro F1\n",
    "    preds = (pf >= best_thr.reshape(1, -1)).astype(int)\n",
    "    macro = np.mean([f1_score(lf[:, c], preds[:, c], average=\"binary\", zero_division=0)\n",
    "                     for c in range(num_channels)])\n",
    "    return best_thr, macro\n",
    "\n",
    "\n",
    "best_thr, best_f1_dev = optimize_thresholds(trainer, dev_split)\n",
    "print(f\"\\nOptimized thresholds: {best_thr}\")\n",
    "print(f\"Dev macro-F1: {best_f1_dev:.4f}\")\n",
    "thresholds = best_thr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b723a53",
   "metadata": {},
   "source": [
    "## 11. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b62413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Token-level ──\n",
    "print(\"Evaluating on test set (token-level)...\")\n",
    "test_metrics = trainer.evaluate(test_split)\n",
    "\n",
    "print(f\"\\nTest results (optimized thresholds):\")\n",
    "print(f\"  Macro F1:      {test_metrics['eval_f1_macro']:.4f}\")\n",
    "print(f\"  Precision:     {test_metrics['eval_precision_macro']:.4f}\")\n",
    "print(f\"  Recall:        {test_metrics['eval_recall_macro']:.4f}\")\n",
    "print(f\"  Hamming Loss:  {test_metrics['eval_hamming_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dfa20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Entity-level (span) evaluation ──\n",
    "\n",
    "def extract_spans(bio_preds):\n",
    "    \"\"\"Extract (entity_type, start, end) spans from BIO predictions.\"\"\"\n",
    "    pairs = {0: 1, 2: 3, 4: 5}  # B→I channel pairs\n",
    "    spans = []\n",
    "    for b_idx, i_idx in pairs.items():\n",
    "        etype = channels[b_idx].replace(\"B-\", \"\")\n",
    "        in_span, start = False, -1\n",
    "        for t in range(len(bio_preds)):\n",
    "            if bio_preds[t, b_idx] == 1:\n",
    "                if in_span:\n",
    "                    spans.append((etype, start, t))\n",
    "                start, in_span = t, True\n",
    "            elif bio_preds[t, i_idx] == 1 and in_span:\n",
    "                continue\n",
    "            else:\n",
    "                if in_span:\n",
    "                    spans.append((etype, start, t))\n",
    "                    in_span = False\n",
    "        if in_span:\n",
    "            spans.append((etype, start, len(bio_preds)))\n",
    "    return spans\n",
    "\n",
    "\n",
    "def compute_entity_metrics(trainer, dataset, thresholds):\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "    all_pred, all_gold = [], []\n",
    "    ex_idx = 0\n",
    "    for batch in trainer.get_eval_dataloader(dataset):\n",
    "        with torch.no_grad():\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n",
    "            probs = torch.sigmoid(model(**inputs).logits).cpu().numpy()\n",
    "        for i in range(labels.shape[0]):\n",
    "            valid = labels[i, :, 0] != -100.0\n",
    "            p = (probs[i][valid] >= thresholds).astype(int)\n",
    "            g = labels[i][valid].astype(int)\n",
    "            all_pred.extend([(ex_idx, *s) for s in extract_spans(p)])\n",
    "            all_gold.extend([(ex_idx, *s) for s in extract_spans(g)])\n",
    "            ex_idx += 1\n",
    "\n",
    "    print(\"\\nEntity-level evaluation:\")\n",
    "    print(\"-\" * 60)\n",
    "    for etype in [\"GEN\", \"UNFAIR\", \"STEREO\"]:\n",
    "        ps = set(s for s in all_pred if s[1] == etype)\n",
    "        gs = set(s for s in all_gold if s[1] == etype)\n",
    "        tp = len(ps & gs)\n",
    "        p = tp / max(len(ps), 1)\n",
    "        r = tp / max(len(gs), 1)\n",
    "        f1 = 2 * p * r / max(p + r, 1e-8)\n",
    "        print(f\"  {etype:8s}: F1={f1:.4f}  P={p:.4f}  R={r:.4f}  (support={len(gs)})\")\n",
    "\n",
    "    ps, gs = set(all_pred), set(all_gold)\n",
    "    tp = len(ps & gs)\n",
    "    micro_p = tp / max(len(ps), 1)\n",
    "    micro_r = tp / max(len(gs), 1)\n",
    "    micro_f1 = 2 * micro_p * micro_r / max(micro_p + micro_r, 1e-8)\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"  {'MICRO':8s}: F1={micro_f1:.4f}  P={micro_p:.4f}  R={micro_r:.4f}\")\n",
    "    return {\"micro_f1\": micro_f1, \"micro_p\": micro_p, \"micro_r\": micro_r}\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENTITY-LEVEL EVALUATION (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "entity_metrics = compute_entity_metrics(trainer, test_split, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca36746",
   "metadata": {},
   "source": [
    "## 12. Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bias(text, model, tokenizer, thresholds, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    inputs = tokenizer(\n",
    "        text, return_tensors=\"pt\", truncation=True,\n",
    "        max_length=128, padding=True,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(model(**inputs).logits[0]).cpu().numpy()\n",
    "    preds = (probs >= thresholds).astype(int)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "    results = []\n",
    "    for i, (tok, pred) in enumerate(zip(tokens, preds)):\n",
    "        active = [channels[j] for j in range(num_channels) if pred[j] == 1]\n",
    "        if active:\n",
    "            clean = tok.replace(\"Ġ\", \"\").replace(\"Ċ\", \"\")\n",
    "            results.append({\"token\": clean, \"labels\": active})\n",
    "    return results\n",
    "\n",
    "\n",
    "examples = [\n",
    "    \"Women are naturally better at nursing.\",\n",
    "    \"All politicians are corrupt liars.\",\n",
    "    \"Young people these days are so lazy and entitled.\",\n",
    "    \"The engineer fixed the problem quickly.\",\n",
    "]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE DEMO (GPT-2)\")\n",
    "print(\"=\" * 60)\n",
    "for text in examples:\n",
    "    print(f\"\\nInput: '{text}'\")\n",
    "    biases = predict_bias(text, model, tokenizer, thresholds, device)\n",
    "    if biases:\n",
    "        for b in biases:\n",
    "            print(f\"  → '{b['token']}': {', '.join(b['labels'])}\")\n",
    "    else:\n",
    "        print(\"  → No bias detected (Neutral)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c8b614",
   "metadata": {},
   "source": [
    "## 13. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./gus-net-gpt2-final\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "np.save(f\"{output_dir}/optimized_thresholds.npy\", thresholds)\n",
    "\n",
    "print(f\"Model, tokenizer and thresholds saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a5cbe",
   "metadata": {},
   "source": [
    "## 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad784e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store causal results for final comparison\n",
    "causal_results = {\n",
    "    \"token_f1\": test_metrics[\"eval_f1_macro\"],\n",
    "    \"token_precision\": test_metrics[\"eval_precision_macro\"],\n",
    "    \"token_recall\": test_metrics[\"eval_recall_macro\"],\n",
    "    \"token_hamming\": test_metrics[\"eval_hamming_loss\"],\n",
    "    \"entity_micro_f1\": entity_metrics[\"micro_f1\"],\n",
    "    \"entity_micro_p\": entity_metrics[\"micro_p\"],\n",
    "    \"entity_micro_r\": entity_metrics[\"micro_r\"],\n",
    "    \"thresholds\": thresholds.copy(),\n",
    "}\n",
    "\n",
    "print(\"Causal GPT-2 results saved for comparison.\")\n",
    "print(f\"  Token Macro F1:   {causal_results['token_f1']:.4f}\")\n",
    "print(f\"  Entity Micro F1:  {causal_results['entity_micro_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ck2d1djpukp",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B — GPT-2 Bidirectional Mode\n",
    "\n",
    "GPT-2 uses a **causal (triangular) attention mask** — each token only attends to\n",
    "tokens to its **left**. This is ideal for language generation but a disadvantage for\n",
    "NER, where right context helps disambiguate tokens.\n",
    "\n",
    "By filling the causal mask bias with `True`, we remove the triangular constraint and\n",
    "let every token attend to every other token — effectively making GPT-2 **bidirectional**\n",
    "(similar to BERT).\n",
    "\n",
    "This **breaks the pre-training assumption** (the model was pre-trained with causal\n",
    "attention), but fine-tuning can adapt the attention heads to use the full context.\n",
    "\n",
    "Below we retrain from scratch with the same hyperparameters to measure the impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q749k6dtsz7",
   "metadata": {},
   "source": [
    "## B.1 Free Causal Model & Create Bidirectional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3t6vj2yldo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal model freed from memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal mask DISABLED — GPT-2 is now bidirectional.\n",
      "Parameters: 124,444,422\n"
     ]
    }
   ],
   "source": [
    "# ── Free causal model from GPU ──\n",
    "del model, trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Causal model freed from memory.\")\n",
    "\n",
    "# ── Fresh GPT-2 with bidirectional attention ──\n",
    "config_bi = AutoConfig.from_pretrained(\"gpt2\")\n",
    "config_bi.num_labels = num_channels\n",
    "config_bi.problem_type = \"multi_label_classification\"\n",
    "config_bi.pad_token_id = tokenizer.pad_token_id\n",
    "config_bi.classifier_dropout = 0.3\n",
    "config_bi.resid_pdrop = 0.15\n",
    "config_bi.embd_pdrop = 0.15\n",
    "config_bi.attn_pdrop = 0.15\n",
    "\n",
    "model_bi = GPT2ForTokenClassification.from_pretrained(\"gpt2\", config=config_bi)\n",
    "model_bi.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# ── Disable causal mask → bidirectional ──\n",
    "for block in model_bi.transformer.h:\n",
    "    # block.attn.bias is the causal mask (lower-triangular).\n",
    "    # Filling with True lets every position attend to every other position.\n",
    "    block.attn.bias.fill_(True)\n",
    "\n",
    "print(\"Causal mask DISABLED — GPT-2 is now bidirectional.\")\n",
    "print(f\"Parameters: {model_bi.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9wzy0n948uf",
   "metadata": {},
   "source": [
    "## B.2 Training (Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "wild9o6oumh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidirectional trainer ready (same hyperparameters as causal).\n"
     ]
    }
   ],
   "source": [
    "training_args_bi = TrainingArguments(\n",
    "    output_dir=\"./gus-net-gpt2-bidirectional\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Reset thresholds for bidirectional model\n",
    "thresholds = np.array([0.5] * num_channels, dtype=np.float32)\n",
    "\n",
    "trainer_bi = FocalLossTrainerGPT2(\n",
    "    model=model_bi,\n",
    "    args=training_args_bi,\n",
    "    train_dataset=train_split,\n",
    "    eval_dataset=dev_split,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    alpha_channel=alpha_channel,\n",
    "    gamma=2.0,\n",
    "    label_smoothing=0.05,\n",
    "    llrd_decay_factor=0.85,\n",
    "    classifier_lr=2e-4,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")\n",
    "\n",
    "print(\"Bidirectional trainer ready (same hyperparameters as causal).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ihmvss0lxp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting bidirectional training...\n",
      "LLRD optimizer (GPT-2):\n",
      "  Classifier LR:     0.0002\n",
      "  Top layer LR:      5e-05\n",
      "  Bottom layer LR:   8.37e-06\n",
      "  Embeddings LR:     7.11e-06\n",
      "Cosine scheduler: 164 warmup / 1640 total\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26595683f447437eb48076358aedf094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1212, 'learning_rate': 5.853658536585366e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b21d887ffc4cb29447bc743429a437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anoca\\AppData\\Local\\Temp\\ipykernel_18820\\2152643530.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-predictions))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.009381568990647793, 'eval_f1_macro': 0.03204712485758974, 'eval_precision_macro': 0.20303992388832323, 'eval_recall_macro': 0.027388654351307756, 'eval_hamming_loss': 0.07959542656112577, 'eval_runtime': 56.4962, 'eval_samples_per_second': 9.93, 'eval_steps_per_second': 0.637, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0106, 'learning_rate': 0.00011951219512195122, 'epoch': 1.22}\n",
      "{'loss': 0.0085, 'learning_rate': 0.0001804878048780488, 'epoch': 1.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073451122726415dae1f9ce90dbf284a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.007518548984080553, 'eval_f1_macro': 0.04111549309123349, 'eval_precision_macro': 0.3333709273182957, 'eval_recall_macro': 0.02383874663775426, 'eval_hamming_loss': 0.06868301899084661, 'eval_runtime': 56.5279, 'eval_samples_per_second': 9.924, 'eval_steps_per_second': 0.637, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.008, 'learning_rate': 0.00019973826287588464, 'epoch': 2.44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87793cb3a8ab4700906f517e53825585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.006969396024942398, 'eval_f1_macro': 0.06088337713648617, 'eval_precision_macro': 0.3887381636344292, 'eval_recall_macro': 0.04070803775096044, 'eval_hamming_loss': 0.06643538877487866, 'eval_runtime': 56.0871, 'eval_samples_per_second': 10.002, 'eval_steps_per_second': 0.642, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0075, 'learning_rate': 0.0001984059629273457, 'epoch': 3.05}\n",
      "{'loss': 0.0071, 'learning_rate': 0.00019596019297180145, 'epoch': 3.66}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b16c61ab6742deae1accf5a348e907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.006604738533496857, 'eval_f1_macro': 0.07464292206353153, 'eval_precision_macro': 0.327238944630249, 'eval_recall_macro': 0.05207316759344267, 'eval_hamming_loss': 0.06610964526531808, 'eval_runtime': 55.892, 'eval_samples_per_second': 10.037, 'eval_steps_per_second': 0.644, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.007, 'learning_rate': 0.00019242862705875577, 'epoch': 4.27}\n",
      "{'loss': 0.0069, 'learning_rate': 0.00018785122509109426, 'epoch': 4.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfb144d5f2641f09b099f19407603ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.006280513945966959, 'eval_f1_macro': 0.07486101872242872, 'eval_precision_macro': 0.3498623035007253, 'eval_recall_macro': 0.04969812177446492, 'eval_hamming_loss': 0.06555588129906512, 'eval_runtime': 55.909, 'eval_samples_per_second': 10.034, 'eval_steps_per_second': 0.644, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0062, 'learning_rate': 0.00018227978067612868, 'epoch': 5.49}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348663d5fd7b445ba52bd4488cd71e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005899177398532629, 'eval_f1_macro': 0.11405044681108815, 'eval_precision_macro': 0.44109679293721227, 'eval_recall_macro': 0.07718448689506617, 'eval_hamming_loss': 0.0648066712270758, 'eval_runtime': 56.0324, 'eval_samples_per_second': 10.012, 'eval_steps_per_second': 0.642, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0065, 'learning_rate': 0.00017577733507749007, 'epoch': 6.1}\n",
      "{'loss': 0.0059, 'learning_rate': 0.00016841746389904304, 'epoch': 6.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8873ed1fa3433398ab4a82fbd43664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005661883857101202, 'eval_f1_macro': 0.2046087324393708, 'eval_precision_macro': 0.511397742687098, 'eval_recall_macro': 0.1476145484252211, 'eval_hamming_loss': 0.06553959412358709, 'eval_runtime': 56.0103, 'eval_samples_per_second': 10.016, 'eval_steps_per_second': 0.643, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.006, 'learning_rate': 0.0001602834445720413, 'epoch': 7.32}\n",
      "{'loss': 0.0057, 'learning_rate': 0.0001514673140654609, 'epoch': 7.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18543fabaef54fbd94b6346cdc14d728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005395388696342707, 'eval_f1_macro': 0.23433040953754017, 'eval_precision_macro': 0.5701048411881858, 'eval_recall_macro': 0.1658618966822866, 'eval_hamming_loss': 0.06303136909997069, 'eval_runtime': 58.4746, 'eval_samples_per_second': 9.594, 'eval_steps_per_second': 0.616, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0056, 'learning_rate': 0.0001420688274815834, 'epoch': 8.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf8862e41cb4e6b9457d44efe744f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005251815542578697, 'eval_f1_macro': 0.24415355806369102, 'eval_precision_macro': 0.5680875859001535, 'eval_recall_macro': 0.17046940581626582, 'eval_hamming_loss': 0.062412456431805596, 'eval_runtime': 55.8304, 'eval_samples_per_second': 10.048, 'eval_steps_per_second': 0.645, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0054, 'learning_rate': 0.00013219432932038712, 'epoch': 9.15}\n",
      "{'loss': 0.0054, 'learning_rate': 0.00012195555018446599, 'epoch': 9.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadd09dac3ab4efab2178b7adcae69ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005169693846255541, 'eval_f1_macro': 0.2735190447103653, 'eval_precision_macro': 0.5675943884078787, 'eval_recall_macro': 0.20044055577789585, 'eval_hamming_loss': 0.06205413857128897, 'eval_runtime': 55.9794, 'eval_samples_per_second': 10.022, 'eval_steps_per_second': 0.643, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0054, 'learning_rate': 0.00011146834253984006, 'epoch': 10.37}\n",
      "{'loss': 0.0053, 'learning_rate': 0.00010085136983760677, 'epoch': 10.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1555eaaceb644de6bcffb864caeab2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005161278415471315, 'eval_f1_macro': 0.31485292028047573, 'eval_precision_macro': 0.5376139170394071, 'eval_recall_macro': 0.23913452103191565, 'eval_hamming_loss': 0.0625264666601518, 'eval_runtime': 56.188, 'eval_samples_per_second': 9.984, 'eval_steps_per_second': 0.641, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0052, 'learning_rate': 9.022476382910982e-05, 'epoch': 11.59}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047f954d6c06422cbfe72386d5394079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005049669183790684, 'eval_f1_macro': 0.315449525693348, 'eval_precision_macro': 0.5471163451847674, 'eval_recall_macro': 0.23936118398999576, 'eval_hamming_loss': 0.06257532818658589, 'eval_runtime': 56.3902, 'eval_samples_per_second': 9.949, 'eval_steps_per_second': 0.638, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0051, 'learning_rate': 7.970876526719333e-05, 'epoch': 12.2}\n",
      "{'loss': 0.0052, 'learning_rate': 6.942236337409622e-05, 'epoch': 12.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cd6748a65045649b14f01c24265d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005014353897422552, 'eval_f1_macro': 0.32038838781788087, 'eval_precision_macro': 0.5609624335487331, 'eval_recall_macro': 0.24468338975072393, 'eval_hamming_loss': 0.06189126681650868, 'eval_runtime': 56.0816, 'eval_samples_per_second': 10.003, 'eval_steps_per_second': 0.642, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0051, 'learning_rate': 5.9481949470499255e-05, 'epoch': 13.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ceab653b134773b61028be4b3db0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0049743917770683765, 'eval_f1_macro': 0.32993247288580646, 'eval_precision_macro': 0.5705957049024466, 'eval_recall_macro': 0.2514932465280891, 'eval_hamming_loss': 0.061646959184338254, 'eval_runtime': 56.7907, 'eval_samples_per_second': 9.878, 'eval_steps_per_second': 0.634, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0051, 'learning_rate': 5.000000000000002e-05, 'epoch': 14.02}\n",
      "{'loss': 0.005, 'learning_rate': 4.108380385068289e-05, 'epoch': 14.63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92d9a590fd542f981cbb71008b56ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004941117484122515, 'eval_f1_macro': 0.31355721686043164, 'eval_precision_macro': 0.5959559622760103, 'eval_recall_macro': 0.2321657399926543, 'eval_hamming_loss': 0.06062086712922245, 'eval_runtime': 56.4177, 'eval_samples_per_second': 9.944, 'eval_steps_per_second': 0.638, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0051, 'learning_rate': 3.283424837422355e-05, 'epoch': 15.24}\n",
      "{'loss': 0.0052, 'learning_rate': 2.5344677838803733e-05, 'epoch': 15.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6004ec4b274edcb241130ff1ef0163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004926861729472876, 'eval_f1_macro': 0.3280018381527943, 'eval_precision_macro': 0.5824266325365969, 'eval_recall_macro': 0.24773459449121882, 'eval_hamming_loss': 0.06097918498973908, 'eval_runtime': 57.0752, 'eval_samples_per_second': 9.829, 'eval_steps_per_second': 0.631, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0048, 'learning_rate': 1.8699837232516227e-05, 'epoch': 16.46}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d8a3258a45471987541608e98a4793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004918945021927357, 'eval_f1_macro': 0.33512640271048916, 'eval_precision_macro': 0.5785517461840494, 'eval_recall_macro': 0.2551600196537773, 'eval_hamming_loss': 0.06122349262190951, 'eval_runtime': 56.0161, 'eval_samples_per_second': 10.015, 'eval_steps_per_second': 0.643, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0051, 'learning_rate': 1.2974913368195695e-05, 'epoch': 17.07}\n",
      "{'loss': 0.005, 'learning_rate': 8.234684139637205e-06, 'epoch': 17.68}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5491dc166e400c84a3f02b986f5f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004913683515042067, 'eval_f1_macro': 0.3291431309093964, 'eval_precision_macro': 0.5911900390495181, 'eval_recall_macro': 0.24738983419598573, 'eval_hamming_loss': 0.06071859018209062, 'eval_runtime': 55.929, 'eval_samples_per_second': 10.031, 'eval_steps_per_second': 0.644, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.005, 'learning_rate': 4.53278555542519e-06, 'epoch': 18.29}\n",
      "{'loss': 0.005, 'learning_rate': 1.9111048439335977e-06, 'epoch': 18.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f3b407036c4735a5ed285cf01cc5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004912855103611946, 'eval_f1_macro': 0.3322603871657897, 'eval_precision_macro': 0.5871754223589785, 'eval_recall_macro': 0.2508063300533998, 'eval_hamming_loss': 0.060832600410436824, 'eval_runtime': 55.8602, 'eval_samples_per_second': 10.043, 'eval_steps_per_second': 0.644, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0051, 'learning_rate': 3.99306496554519e-07, 'epoch': 19.51}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c74936ea75c462691d036cc9c789023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004913104698061943, 'eval_f1_macro': 0.33211686178296024, 'eval_precision_macro': 0.5850317810467742, 'eval_recall_macro': 0.25097895218355687, 'eval_hamming_loss': 0.060946610638783025, 'eval_runtime': 56.4172, 'eval_samples_per_second': 9.944, 'eval_steps_per_second': 0.638, 'epoch': 20.0}\n",
      "{'train_runtime': 14268.1446, 'train_samples_per_second': 3.668, 'train_steps_per_second': 0.115, 'train_loss': 0.009423801802643916, 'epoch': 20.0}\n",
      "\n",
      "Training loss: 0.0094\n",
      "Runtime:       14268.1s\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting bidirectional training...\")\n",
    "train_result_bi = trainer_bi.train()\n",
    "\n",
    "print(f\"\\nTraining loss: {train_result_bi.training_loss:.4f}\")\n",
    "print(f\"Runtime:       {train_result_bi.metrics['train_runtime']:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026xjo6xo35v",
   "metadata": {},
   "source": [
    "## B.3 SWA (Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "j4zu4e1r3oe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWA: averaging 5 checkpoints\n",
      "SWA applied (5 checkpoints).\n"
     ]
    }
   ],
   "source": [
    "apply_swa(trainer_bi, \"./gus-net-gpt2-bidirectional\", last_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aoiutf7hr",
   "metadata": {},
   "source": [
    "## B.4 Threshold Optimization (Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "guwjb2k4kw9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 1 — grid search:\n",
      "  B-GEN     : thr=0.375, F1=0.5344\n",
      "  I-GEN     : thr=0.375, F1=0.4035\n",
      "  B-UNFAIR  : thr=0.375, F1=0.4438\n",
      "  I-UNFAIR  : thr=0.350, F1=0.3206\n",
      "  B-STEREO  : thr=0.450, F1=0.4515\n",
      "  I-STEREO  : thr=0.425, F1=0.5535\n",
      "\n",
      "Pass 2 — refinement:\n",
      "  B-GEN     : thr=0.3866, F1=0.5401\n",
      "  I-GEN     : thr=0.3750, F1=0.4004\n",
      "  B-UNFAIR  : thr=0.3873, F1=0.4505\n",
      "  I-UNFAIR  : thr=0.3466, F1=0.3287\n",
      "  B-STEREO  : thr=0.4500, F1=0.4497\n",
      "  I-STEREO  : thr=0.4315, F1=0.5561\n",
      "\n",
      "Optimized thresholds (bidirectional): [0.3866261  0.375      0.38731122 0.34655127 0.45       0.43150443]\n",
      "Dev macro-F1: 0.4551\n"
     ]
    }
   ],
   "source": [
    "best_thr_bi, best_f1_dev_bi = optimize_thresholds(trainer_bi, dev_split)\n",
    "print(f\"\\nOptimized thresholds (bidirectional): {best_thr_bi}\")\n",
    "print(f\"Dev macro-F1: {best_f1_dev_bi:.4f}\")\n",
    "thresholds = best_thr_bi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dm4qtw9jmhd",
   "metadata": {},
   "source": [
    "## B.5 Evaluation (Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "xsp1yg58luh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating bidirectional model on test set (token-level)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c607eb7c124202b8dec1822365a109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bidirectional test results (optimized thresholds):\n",
      "  Macro F1:      0.4349\n",
      "  Precision:     0.4062\n",
      "  Recall:        0.4973\n",
      "  Hamming Loss:  0.0827\n"
     ]
    }
   ],
   "source": [
    "# ── Token-level ──\n",
    "print(\"Evaluating bidirectional model on test set (token-level)...\")\n",
    "test_metrics_bi = trainer_bi.evaluate(test_split)\n",
    "\n",
    "print(f\"\\nBidirectional test results (optimized thresholds):\")\n",
    "print(f\"  Macro F1:      {test_metrics_bi['eval_f1_macro']:.4f}\")\n",
    "print(f\"  Precision:     {test_metrics_bi['eval_precision_macro']:.4f}\")\n",
    "print(f\"  Recall:        {test_metrics_bi['eval_recall_macro']:.4f}\")\n",
    "print(f\"  Hamming Loss:  {test_metrics_bi['eval_hamming_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "lwqe6n3mbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENTITY-LEVEL EVALUATION — Bidirectional (Test Set)\n",
      "============================================================\n",
      "\n",
      "Entity-level evaluation:\n",
      "------------------------------------------------------------\n",
      "  GEN     : F1=0.3344  P=0.2913  R=0.3927  (support=764)\n",
      "  UNFAIR  : F1=0.2457  P=0.2535  R=0.2384  (support=151)\n",
      "  STEREO  : F1=0.1265  P=0.1576  R=0.1057  (support=246)\n",
      "------------------------------------------------------------\n",
      "  MICRO   : F1=0.2898  P=0.2708  R=0.3118\n"
     ]
    }
   ],
   "source": [
    "# ── Entity-level ──\n",
    "print(\"=\" * 60)\n",
    "print(\"ENTITY-LEVEL EVALUATION — Bidirectional (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "entity_metrics_bi = compute_entity_metrics(trainer_bi, test_split, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gnvyxot0gqq",
   "metadata": {},
   "source": [
    "## B.6 Inference Demo (Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kgesva5g88",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE DEMO (GPT-2 Bidirectional)\")\n",
    "print(\"=\" * 60)\n",
    "for text in examples:\n",
    "    print(f\"\\nInput: '{text}'\")\n",
    "    biases = predict_bias(text, model_bi, tokenizer, thresholds, device)\n",
    "    if biases:\n",
    "        for b in biases:\n",
    "            print(f\"  → '{b['token']}': {', '.join(b['labels'])}\")\n",
    "    else:\n",
    "        print(\"  → No bias detected (Neutral)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82er4ce6ol",
   "metadata": {},
   "source": [
    "## B.7 Save Bidirectional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "p5xoj6ehp1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidirectional model saved to ./gus-net-gpt2-bidirectional-final\n"
     ]
    }
   ],
   "source": [
    "output_dir_bi = \"./gus-net-gpt2-bidirectional-final\"\n",
    "trainer_bi.save_model(output_dir_bi)\n",
    "tokenizer.save_pretrained(output_dir_bi)\n",
    "np.save(f\"{output_dir_bi}/optimized_thresholds.npy\", thresholds)\n",
    "\n",
    "print(f\"Bidirectional model saved to {output_dir_bi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44xi3f5erds",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparison: Causal vs Bidirectional GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11wv5zy8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_results = {\n",
    "    \"token_f1\": test_metrics_bi[\"eval_f1_macro\"],\n",
    "    \"token_precision\": test_metrics_bi[\"eval_precision_macro\"],\n",
    "    \"token_recall\": test_metrics_bi[\"eval_recall_macro\"],\n",
    "    \"token_hamming\": test_metrics_bi[\"eval_hamming_loss\"],\n",
    "    \"entity_micro_f1\": entity_metrics_bi[\"micro_f1\"],\n",
    "    \"entity_micro_p\": entity_metrics_bi[\"micro_p\"],\n",
    "    \"entity_micro_r\": entity_metrics_bi[\"micro_r\"],\n",
    "    \"thresholds\": thresholds.copy(),\n",
    "}\n",
    "\n",
    "# ── Comparison table ──\n",
    "def delta(bi, ca):\n",
    "    d = bi - ca\n",
    "    return f\"{d:+.4f}\" if d != 0 else \"  0.0000\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: Causal vs Bidirectional GPT-2\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<25s} {'Causal':>10s} {'Bidirect.':>10s} {'Δ':>10s}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "rows = [\n",
    "    (\"Token Macro F1\",    \"token_f1\"),\n",
    "    (\"Token Precision\",   \"token_precision\"),\n",
    "    (\"Token Recall\",      \"token_recall\"),\n",
    "    (\"Token Hamming Loss\",\"token_hamming\"),\n",
    "    (\"Entity Micro F1\",   \"entity_micro_f1\"),\n",
    "    (\"Entity Precision\",  \"entity_micro_p\"),\n",
    "    (\"Entity Recall\",     \"entity_micro_r\"),\n",
    "]\n",
    "\n",
    "for label, key in rows:\n",
    "    c = causal_results[key]\n",
    "    b = bidirectional_results[key]\n",
    "    print(f\"  {label:<23s} {c:>10.4f} {b:>10.4f} {delta(b, c):>10s}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Winner\n",
    "c_f1 = causal_results[\"token_f1\"]\n",
    "b_f1 = bidirectional_results[\"token_f1\"]\n",
    "winner = \"Bidirectional\" if b_f1 > c_f1 else \"Causal\" if c_f1 > b_f1 else \"Tie\"\n",
    "margin = abs(b_f1 - c_f1)\n",
    "print(f\"\\n  Winner (Token F1): {winner} ({margin:+.4f})\")\n",
    "\n",
    "c_ef1 = causal_results[\"entity_micro_f1\"]\n",
    "b_ef1 = bidirectional_results[\"entity_micro_f1\"]\n",
    "winner_e = \"Bidirectional\" if b_ef1 > c_ef1 else \"Causal\" if c_ef1 > b_ef1 else \"Tie\"\n",
    "margin_e = abs(b_ef1 - c_ef1)\n",
    "print(f\"  Winner (Entity F1): {winner_e} ({margin_e:+.4f})\")\n",
    "\n",
    "print(\"\\nOptimized thresholds:\")\n",
    "print(f\"  {'Channel':<12s} {'Causal':>10s} {'Bidirect.':>10s}\")\n",
    "for i, ch in enumerate(channels):\n",
    "    print(f\"  {ch:<12s} {causal_results['thresholds'][i]:>10.4f} {bidirectional_results['thresholds'][i]:>10.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
