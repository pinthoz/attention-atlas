{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUS-Net: Social Bias Classification with Generalizations, Unfairness, and Stereotypes\n",
    "\n",
    "## Abstract\n",
    "This notebook documents the implementation and utilization of **GUS-Net**, a specialized Named Entity Recognition (NER) model designed to detect social biases in text. The model focuses on three specific categories of bias:\n",
    "1.  **Generalizations (GEN)**: Categorical statements that attribute properties to a group without exception.\n",
    "2.  **Unfairness (UNFAIR)**: Language that is inherently unjust, pejorative, or discriminatory.\n",
    "3.  **Stereotypes (STEREO)**: Attribution of fixed characteristics to particular social groups.\n",
    "\n",
    "The methodology encompasses three primary approaches:\n",
    "1.  **Inference with Pre-trained Models**: Utilizing the HuggingFace `ethical-spectacle/social-bias-ner` model.\n",
    "2.  **Training Methodology**: A comprehensive guide to training the model from scratch using the focal loss function to address class imbalance.\n",
    "3.  **Integration Architecture**: A proposed framework for integrating GUS-Net into the **Attention Atlas** visualization tool.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Methodology: Pre-trained Model Inference\n",
    "\n",
    "This section demonstrates how to utilize the pre-trained GUS-Net model for immediate inference. This approach is optimal for rapid testing and integration where custom fine-tuning is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e0d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# 1. Load Model and Tokenizer\n",
    "# The model is hosted on HuggingFace Hub under the Ethical Spectacle organization\n",
    "model_name = \"ethical-spectacle/social-bias-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# 2. Define Label Mappings\n",
    "# The model uses BIO tagging scheme for 3 bias types + Outside (O)\n",
    "id2label = {\n",
    "    0: \"O\",           # Outside (neutral)\n",
    "    1: \"B-STEREO\",    # Begin Stereotype\n",
    "    2: \"I-STEREO\",    # Inside Stereotype\n",
    "    3: \"B-GEN\",       # Begin Generalization\n",
    "    4: \"I-GEN\",       # Inside Generalization\n",
    "    5: \"B-UNFAIR\",    # Begin Unfairness\n",
    "    6: \"I-UNFAIR\"     # Inside Unfairness\n",
    "}\n",
    "\n",
    "def detect_bias(text: str, threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    Detects social bias spans in a given text using the GUS-Net model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to analyze.\n",
    "        threshold (float): Confidence threshold for label acceptance (default: 0.5).\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing biased tokens and their classification metadata.\n",
    "    \"\"\"\n",
    "    # Tokenization with offset mapping to retrieve character positions\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=128,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")[0]\n",
    "    \n",
    "    # Inference step\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    # Apply sigmoid activation for multi-label classification probability\n",
    "    probs = torch.sigmoid(outputs.logits[0])\n",
    "    \n",
    "    # Process results\n",
    "    results = []\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    for i, (token, prob, offset) in enumerate(zip(tokens, probs, offset_mapping)):\n",
    "        # Skip special tokens\n",
    "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "            continue\n",
    "            \n",
    "        # Identify labels exceeding the confidence threshold\n",
    "        labels = []\n",
    "        for label_id, p in enumerate(prob):\n",
    "            if p > threshold and label_id != 0:  # Ignore 'O' label\n",
    "                labels.append({\n",
    "                    \"label\": id2label[label_id],\n",
    "                    \"confidence\": p.item()\n",
    "                })\n",
    "        \n",
    "        if labels:\n",
    "            results.append({\n",
    "                \"token\": token,\n",
    "                \"char_start\": offset[0].item(),\n",
    "                \"char_end\": offset[1].item(),\n",
    "                \"labels\": labels\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- Demonstration ---\n",
    "examples = [\n",
    "    \"Women are naturally better at nursing.\",\n",
    "    \"All politicians are corrupt liars.\",\n",
    "    \"Young people these days are so lazy and entitled.\",\n",
    "    \"The engineer fixed the problem quickly.\"  # Neutral control sentence\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in examples:\n",
    "    print(f\"\\nInput: '{text}'\")\n",
    "    biases = detect_bias(text)\n",
    "    if biases:\n",
    "        for b in biases:\n",
    "            labels_str = \", \".join([f\"{l['label']} ({l['confidence']:.2f})\" for l in b['labels']])\n",
    "            print(f\"  → '{b['token']}': {labels_str}\")\n",
    "    else:\n",
    "        print(\"  → No bias detected (Neutral)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c5ca5f",
   "metadata": {},
   "source": [
    "## 2. Experimental Setup: Fine Tuning\n",
    "\n",
    "This section outlines the procedure for training the GUS-Net model from the ground up utilizing the official dataset. This methodology is applicable for domain adaptation, reproducibility studies, or further academic research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9215fe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install datasets transformers torch scikit-learn numpy\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50cbf8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 78 duplicate samples\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text_str', 'ner_tags', 'rationale', 'individual_ner_annotations_json', 'id', '__index_level_0__'],\n",
      "        num_rows: 3661\n",
      "    })\n",
      "})\n",
      "Number of examples: 3661\n",
      "\n",
      "Sample example:\n",
      "{'text_str': 'Should we assume that every retired individual spends hours playing bingo each week?', 'ner_tags': \"[['O'], ['O'], ['O'], ['O'], ['B-GEN'], ['I-GEN', 'B-STEREO'], ['I-GEN', 'I-STEREO'], ['I-STEREO'], ['I-STEREO'], ['I-STEREO'], ['I-STEREO'], ['I-STEREO'], ['I-STEREO']]\", 'rationale': '[\\'Reasoning: Let\\\\\\'s think step by step in order to produce the annotations. We will analyze each word to determine if it fits the description of a generalization (GEN).\\\\n\\\\n1. \"Should\" - This is an auxiliary verb and does not fit the definition of a generalization. Tag: O\\\\n2. \"we\" - This is a pronoun but not used as a generalization here. Tag: O\\\\n3. \"assume\" - This is a verb and does not fit the definition of a generalization. Tag: O\\\\n4. \"that\" - This is a conjunction and does not fit the definition of a generalization. Tag: O\\\\n5. \"every\" - This is an adjective that can be part of a generalization. Tag: B-GEN\\\\n6. \"retired\" - This is an adjective describing \"individual\" as part of the generalization. Tag: I-GEN\\\\n7. \"individual\" - This is a noun that completes the generalization started by \"every retired\". Tag: I-GEN\\\\n8. \"spends\" - This is a verb and does not fit the definition of a generalization. Tag: O\\\\n9. \"hours\" - This is a noun describing time but is not a generalization. Tag: O\\\\n10. \"playing\" - This is a verb and does not fit the definition of a generalization. Tag: O\\\\n11. \"bingo\" - This is a noun but not a generalization. Tag: O\\\\n12. \"each\" - This is an adjective but in this context, it does not form part of a generalization. Tag: O\\\\n13. \"week\" - This is a noun but in this context, it does not complete a generalization. Tag: O\\', \"Reasoning: Let\\'s think step by step in order to produce the annotations. We need to check each word to determine if it contains any harsh or unjust characterizations or offensive language related to the context of unfairness.\\\\n\\\\n1. **Should**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n2. **we**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n3. **assume**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n4. **that**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n5. **every**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n6. **retired**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n7. **individual**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n8. **spends**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n9. **hours**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n10. **playing**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n11. **bingo**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n12. **each**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n13. **week**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n\\\\nThe sentence does not contain any harsh or unjust characterizations or offensive language. Therefore, all words should be labeled with \\'O\\'.\", \\'Reasoning: Let\\\\\\'s think step by step in order to produce the annotations. We need to carefully analyze each word in the input sentence to determine if it is part of a stereotype statement.\\\\n\\\\n1. \"Should\" - This word does not contribute to a stereotype. Label: O\\\\n2. \"we\" - This word does not contribute to a stereotype. Label: O\\\\n3. \"assume\" - This word does not contribute to a stereotype. Label: O\\\\n4. \"that\" - This word does not contribute to a stereotype. Label: O\\\\n5. \"every\" - This word does not contribute to a stereotype. Label: O\\\\n6. \"retired\" - This word is part of the stereotype suggesting a specific activity associated with retired individuals. Label: B-STEREO\\\\n7. \"individual\" - This word continues the stereotype. Label: I-STEREO\\\\n8. \"spends\" - This word continues the stereotype. Label: I-STEREO\\\\n9. \"hours\" - This word continues the stereotype. Label: I-STEREO\\\\n10. \"playing\" - This word continues the stereotype. Label: I-STEREO\\\\n11. \"bingo\" - This word continues the stereotype. Label: I-STEREO\\\\n12. \"each\" - This word continues the stereotype. Label: I-STEREO\\\\n13. \"week?\" - This word continues the stereotype. Label: I-STEREO\\\\n\\\\nBy labeling each word according to the given entity description, we ensure the stereotype entity is identified correctly.\\']', 'individual_ner_annotations_json': '[Prediction(\\n    rationale=\\'Reasoning: Let\\\\\\'s think step by step in order to produce the annotations. We will analyze each word to determine if it fits the description of a generalization (GEN).\\\\n\\\\n1. \"Should\" - This is an auxiliary verb and does not fit the definition of a generalization. Tag: O\\\\n2. \"we\" - This is a pronoun but not used as a generalization here. Tag: O\\\\n3. \"assume\" - This is a verb and does not fit the definition of a generalization. Tag: O\\\\n4. \"that\" - This is a conjunction and does not fit the definition of a generalization. Tag: O\\\\n5. \"every\" - This is an adjective that can be part of a generalization. Tag: B-GEN\\\\n6. \"retired\" - This is an adjective describing \"individual\" as part of the generalization. Tag: I-GEN\\\\n7. \"individual\" - This is a noun that completes the generalization started by \"every retired\". Tag: I-GEN\\\\n8. \"spends\" - This is a verb and does not fit the definition of a generalization. Tag: O\\\\n9. \"hours\" - This is a noun describing time but is not a generalization. Tag: O\\\\n10. \"playing\" - This is a verb and does not fit the definition of a generalization. Tag: O\\\\n11. \"bingo\" - This is a noun but not a generalization. Tag: O\\\\n12. \"each\" - This is an adjective but in this context, it does not form part of a generalization. Tag: O\\\\n13. \"week\" - This is a noun but in this context, it does not complete a generalization. Tag: O\\',\\n    annotations=\"[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-GEN\\', \\'I-GEN\\', \\'I-GEN\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\']\"\\n), Prediction(\\n    rationale=\"Reasoning: Let\\'s think step by step in order to produce the annotations. We need to check each word to determine if it contains any harsh or unjust characterizations or offensive language related to the context of unfairness.\\\\n\\\\n1. **Should**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n2. **we**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n3. **assume**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n4. **that**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n5. **every**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n6. **retired**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n7. **individual**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n8. **spends**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n9. **hours**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n10. **playing**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n11. **bingo**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n12. **each**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n13. **week**: This word is neutral, no harsh or unjust characterization. Tag: \\'O\\'.\\\\n\\\\nThe sentence does not contain any harsh or unjust characterizations or offensive language. Therefore, all words should be labeled with \\'O\\'.\",\\n    annotations=\"[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\']\"\\n), Prediction(\\n    rationale=\\'Reasoning: Let\\\\\\'s think step by step in order to produce the annotations. We need to carefully analyze each word in the input sentence to determine if it is part of a stereotype statement.\\\\n\\\\n1. \"Should\" - This word does not contribute to a stereotype. Label: O\\\\n2. \"we\" - This word does not contribute to a stereotype. Label: O\\\\n3. \"assume\" - This word does not contribute to a stereotype. Label: O\\\\n4. \"that\" - This word does not contribute to a stereotype. Label: O\\\\n5. \"every\" - This word does not contribute to a stereotype. Label: O\\\\n6. \"retired\" - This word is part of the stereotype suggesting a specific activity associated with retired individuals. Label: B-STEREO\\\\n7. \"individual\" - This word continues the stereotype. Label: I-STEREO\\\\n8. \"spends\" - This word continues the stereotype. Label: I-STEREO\\\\n9. \"hours\" - This word continues the stereotype. Label: I-STEREO\\\\n10. \"playing\" - This word continues the stereotype. Label: I-STEREO\\\\n11. \"bingo\" - This word continues the stereotype. Label: I-STEREO\\\\n12. \"each\" - This word continues the stereotype. Label: I-STEREO\\\\n13. \"week?\" - This word continues the stereotype. Label: I-STEREO\\\\n\\\\nBy labeling each word according to the given entity description, we ensure the stereotype entity is identified correctly.\\',\\n    annotations=\"[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-STEREO\\', \\'I-STEREO\\', \\'I-STEREO\\', \\'I-STEREO\\', \\'I-STEREO\\', \\'I-STEREO\\', \\'I-STEREO\\', \\'I-STEREO\\']\"\\n)]', 'id': 1, '__index_level_0__': 0}\n"
     ]
    }
   ],
   "source": [
    "# Load local standardized dataset\n",
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "# Use the standardized local dataset\n",
    "dataset_path = '../../dataset/new_dataset.json'\n",
    "if not os.path.exists(dataset_path):\n",
    "    # Fallback for different working directories\n",
    "    dataset_path = 'dataset/new_dataset.json'\n",
    "\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    local_data = json.load(f)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_list(local_data['bias_dataset'])\n",
    "})\n",
    "\n",
    "print(f\"Loaded {len(dataset['train'])} examples from {dataset_path}\")\n",
    "print(f\"Dataset features: {dataset['train'].column_names}\")\n",
    "print('\\nSample entry:')\n",
    "print(json.dumps(dataset['train'][0], indent=2, ensure_ascii=False)[:300] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20b6caf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels: 6\n",
      "Channel mapping: {'B-GEN': 0, 'I-GEN': 1, 'B-UNFAIR': 2, 'I-UNFAIR': 3, 'B-STEREO': 4, 'I-STEREO': 5}\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define multi-label channels (no explicit O class; O = all zeros)\n",
    "channels = [\"B-GEN\", \"I-GEN\", \"B-UNFAIR\", \"I-UNFAIR\", \"B-STEREO\", \"I-STEREO\"]\n",
    "channel2idx = {c: i for i, c in enumerate(channels)}\n",
    "idx2channel = {i: c for i, c in enumerate(channels)}\n",
    "num_channels = len(channels)\n",
    "\n",
    "print(f\"Number of channels: {num_channels}\")\n",
    "print(f\"Channel mapping: {channel2idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bb537e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(example):\n",
    "    \"\"\"\n",
    "    Extracts annotations from the standardized dictionary format.\n",
    "    Constructs a list of tags for each word.\n",
    "    \"\"\"\n",
    "    anno_dict = example['annotations']\n",
    "    # Combine tags for each token index\n",
    "    combined = []\n",
    "    keys = ['GEN', 'STEREO', 'UNFAIR']\n",
    "    \n",
    "    # All lists have the same length\n",
    "    length = len(anno_dict['GEN'])\n",
    "    for i in range(length):\n",
    "        tags = [anno_dict[k][i] for k in keys if anno_dict[k][i] != 'O']\n",
    "        if not tags:\n",
    "            tags = ['O']\n",
    "        combined.append(tags)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e027b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_example(example):\n",
    "    \"\"\"\n",
    "    Preprocess an example:\n",
    "    - Tokenize with is_split_into_words=True for word-to-subword alignment\n",
    "    - Use word_ids() to map each subword token back to its original word\n",
    "    - Build multi-hot label matrix [seq_len, num_channels]\n",
    "    - Mask special/padding tokens with -100\n",
    "    \"\"\"\n",
    "    text = example['text_str']\n",
    "    word_tags = parse_annotations(example)  # list of lists, one per word\n",
    "\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Tokenize with word-level alignment\n",
    "    tokenized = tokenizer(\n",
    "        words,\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "    )\n",
    "\n",
    "    word_ids = tokenized.word_ids() \n",
    "    seq_len = len(word_ids)\n",
    "\n",
    "    labels_multi = np.zeros((seq_len, num_channels), dtype=np.float32)\n",
    "\n",
    "    prev_word_id = None\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            prev_word_id = None\n",
    "            continue\n",
    "        if word_id >= len(word_tags):\n",
    "            prev_word_id = word_id\n",
    "            continue\n",
    "        tags = word_tags[word_id]\n",
    "        for tag in tags:\n",
    "            if tag == 'O': continue\n",
    "            if word_id == prev_word_id:\n",
    "                if tag.startswith('B-'):\n",
    "                    i_tag = 'I-' + tag[2:]\n",
    "                    if i_tag in channel2idx:\n",
    "                        labels_multi[idx, channel2idx[i_tag]] = 1.0\n",
    "                elif tag in channel2idx:\n",
    "                    labels_multi[idx, channel2idx[tag]] = 1.0\n",
    "            else:\n",
    "                if tag in channel2idx:\n",
    "                    labels_multi[idx, channel2idx[tag]] = 1.0\n",
    "        prev_word_id = word_id\n",
    "\n",
    "    final_labels = []\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            final_labels.append([-100.0] * num_channels)\n",
    "        else:\n",
    "            final_labels.append(labels_multi[idx].tolist())\n",
    "\n",
    "    tokenized['labels'] = final_labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d444f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1281919edf46a780f299b9249d1d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3661 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete!\n",
      "\n",
      "Sanity check:\n",
      "  Total valid tokens: 68314\n",
      "  Tokens with at least one bias label: 21677\n",
      "  Positive rate: 31.73%\n",
      "  OK — annotations loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    prepare_example,\n",
    "    batched=False,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "print(\"Tokenization complete!\")\n",
    "\n",
    "# === Sanity check: verify annotations survived preprocessing ===\n",
    "total_positive_tokens = 0\n",
    "total_valid_tokens = 0\n",
    "for ex in tokenized_dataset[\"train\"]:\n",
    "    labels = np.array(ex[\"labels\"])\n",
    "    valid = labels[labels[:, 0] != -100.0]\n",
    "    total_valid_tokens += len(valid)\n",
    "    total_positive_tokens += (valid > 0).any(axis=1).sum()\n",
    "\n",
    "print(f\"\\nSanity check:\")\n",
    "print(f\"  Total valid tokens: {total_valid_tokens}\")\n",
    "print(f\"  Tokens with at least one bias label: {total_positive_tokens}\")\n",
    "print(f\"  Positive rate: {total_positive_tokens / max(total_valid_tokens, 1):.2%}\")\n",
    "assert total_positive_tokens > 0, (\n",
    "    \"FATAL: No positive labels found. Check that prepare_example \"\n",
    "    \"is reading from the correct dataset field.\"\n",
    ")\n",
    "print(\"  OK — annotations loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1a829ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2562\n",
      "Dev size:   549\n",
      "Test size:  550\n"
     ]
    }
   ],
   "source": [
    "# Split: 70% train, 15% dev, 15% test (matching paper methodology)\n",
    "\n",
    "# First split: train (70%) vs dev+test (30%)\n",
    "train_devtest = tokenized_dataset[\"train\"].train_test_split(\n",
    "    test_size=0.30, seed=42\n",
    ")\n",
    "train_split = train_devtest[\"train\"]\n",
    "devtest_split = train_devtest[\"test\"]\n",
    "\n",
    "# Second split: dev (50% of 30% = 15%) vs test (50% of 30% = 15%)\n",
    "dev_test = devtest_split.train_test_split(test_size=0.5, seed=42)\n",
    "dev_split = dev_test[\"train\"]\n",
    "test_split = dev_test[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_split)}\")\n",
    "print(f\"Dev size:   {len(dev_split)}\")\n",
    "print(f\"Test size:  {len(test_split)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24e55e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: bert\n",
      "Number of parameters: 108,896,262\n",
      "Classifier dropout: 0.3\n",
      "Hidden dropout: 0.15\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model with increased regularization for small dataset\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "config.num_labels = num_channels\n",
    "config.problem_type = \"multi_label_classification\"\n",
    "config.classifier_dropout = 0.3        # Dropout before classifier head\n",
    "config.hidden_dropout_prob = 0.15      # Slightly increase hidden dropout\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model.config.model_type}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "print(f\"Classifier dropout: {model.config.classifier_dropout}\")\n",
    "print(f\"Hidden dropout: {model.config.hidden_dropout_prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aad75563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel statistics:\n",
      "  B-GEN: 3446 positives, α=0.0835\n",
      "  I-GEN: 3436 positives, α=0.0837\n",
      "  B-UNFAIR: 716 positives, α=0.4018\n",
      "  I-UNFAIR: 1978 positives, α=0.1454\n",
      "  B-STEREO: 1135 positives, α=0.2534\n",
      "  I-STEREO: 8939 positives, α=0.0322\n",
      "\n",
      "Total valid tokens: 47550\n"
     ]
    }
   ],
   "source": [
    "def estimate_channel_frequencies(dataset_split):\n",
    "    \"\"\"\n",
    "    Estimate frequency of positives per channel in the training split.\n",
    "    \"\"\"\n",
    "    positives = np.zeros(num_channels, dtype=np.int64)\n",
    "    total = 0\n",
    "\n",
    "    for example in dataset_split:\n",
    "        labels = np.array(example[\"labels\"])  # [seq_len, num_channels]\n",
    "        # Mask for valid tokens\n",
    "        valid_mask = labels[:, 0] != -100.0\n",
    "        valid_labels = labels[valid_mask]\n",
    "        if valid_labels.size == 0:\n",
    "            continue\n",
    "        positives += valid_labels.sum(axis=0).astype(np.int64)\n",
    "        total += valid_labels.shape[0]\n",
    "\n",
    "    return positives, total\n",
    "\n",
    "\n",
    "# Calculate alpha values for focal loss\n",
    "channel_pos, total_tokens = estimate_channel_frequencies(train_split)\n",
    "# Avoid division by zero: if a channel doesn't appear, assign minimum frequency\n",
    "channel_pos = np.maximum(channel_pos, 1)\n",
    "freq = channel_pos / float(total_tokens)\n",
    "\n",
    "# α_c ∝ 1 / freq_c, normalized\n",
    "inv_freq = 1.0 / freq\n",
    "alpha_channel = inv_freq / inv_freq.sum()\n",
    "alpha_channel = torch.tensor(alpha_channel, dtype=torch.float32)\n",
    "\n",
    "print(\"Channel statistics:\")\n",
    "for i, ch in enumerate(channels):\n",
    "    print(f\"  {ch}: {channel_pos[i]} positives, α={alpha_channel[i]:.4f}\")\n",
    "print(f\"\\nTotal valid tokens: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d874ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossMultiLabel(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal loss applied channel-wise for multi-label classification.\n",
    "    inputs: logits [N, num_channels]\n",
    "    targets: multi-hot [N, num_channels]\n",
    "    alpha: tensor [num_channels]\n",
    "    label_smoothing: smooth targets to prevent overconfident predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha, gamma=2.0, reduction=\"mean\", label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"alpha\", alpha)\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        inputs: logits\n",
    "        targets: multi-hot (0/1)\n",
    "        \"\"\"\n",
    "        # Label smoothing: y_smooth = y * (1 - eps) + 0.5 * eps\n",
    "        if self.label_smoothing > 0:\n",
    "            targets = targets * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "\n",
    "        bce = F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets.float(), reduction=\"none\"\n",
    "        )  # [N, C]\n",
    "        pt = torch.exp(-bce)\n",
    "        # Broadcasting alpha: [C] -> [N, C]\n",
    "        focal = self.alpha.to(inputs.device) * (1 - pt) ** self.gamma * bce\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal.sum()\n",
    "        else:\n",
    "            return focal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f1e60af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "class FocalLossTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Trainer with:\n",
    "    - Focal Loss for multi-label classification (with label smoothing)\n",
    "    - Layer-wise Learning Rate Decay (LLRD) for BERT fine-tuning\n",
    "    - Cosine annealing scheduler with warmup\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, alpha_channel, gamma=2.0, label_smoothing=0.0,\n",
    "                 llrd_decay_factor=0.85, classifier_lr=2e-4, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.focal_loss = FocalLossMultiLabel(\n",
    "            alpha=alpha_channel, gamma=gamma, label_smoothing=label_smoothing\n",
    "        )\n",
    "        self.llrd_decay_factor = llrd_decay_factor\n",
    "        self.classifier_lr = classifier_lr\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        \"\"\"Layer-wise Learning Rate Decay: higher LR for top layers, lower for bottom.\"\"\"\n",
    "        base_lr = self.args.learning_rate\n",
    "        decay = self.llrd_decay_factor\n",
    "        no_decay_keys = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "\n",
    "        opt_params = []\n",
    "\n",
    "        # 1. Classifier head: highest LR\n",
    "        opt_params.append({\n",
    "            \"params\": [p for n, p in self.model.named_parameters() if \"classifier\" in n],\n",
    "            \"lr\": self.classifier_lr,\n",
    "            \"weight_decay\": 0.0,\n",
    "        })\n",
    "\n",
    "        # 2. BERT encoder layers 11 -> 0: progressively lower LR\n",
    "        for layer_idx in range(11, -1, -1):\n",
    "            layer_lr = base_lr * (decay ** (11 - layer_idx))\n",
    "            layer_decay = []\n",
    "            layer_no_decay = []\n",
    "\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if f\"bert.encoder.layer.{layer_idx}.\" in n:\n",
    "                    if any(nd in n for nd in no_decay_keys):\n",
    "                        layer_no_decay.append(p)\n",
    "                    else:\n",
    "                        layer_decay.append(p)\n",
    "\n",
    "            if layer_decay:\n",
    "                opt_params.append({\n",
    "                    \"params\": layer_decay,\n",
    "                    \"lr\": layer_lr,\n",
    "                    \"weight_decay\": self.args.weight_decay,\n",
    "                })\n",
    "            if layer_no_decay:\n",
    "                opt_params.append({\n",
    "                    \"params\": layer_no_decay,\n",
    "                    \"lr\": layer_lr,\n",
    "                    \"weight_decay\": 0.0,\n",
    "                })\n",
    "\n",
    "        # 3. Embeddings: lowest LR\n",
    "        emb_lr = base_lr * (decay ** 12)\n",
    "        emb_decay = []\n",
    "        emb_no_decay = []\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if \"bert.embeddings\" in n:\n",
    "                if any(nd in n for nd in no_decay_keys):\n",
    "                    emb_no_decay.append(p)\n",
    "                else:\n",
    "                    emb_decay.append(p)\n",
    "\n",
    "        if emb_decay:\n",
    "            opt_params.append({\n",
    "                \"params\": emb_decay,\n",
    "                \"lr\": emb_lr,\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "            })\n",
    "        if emb_no_decay:\n",
    "            opt_params.append({\n",
    "                \"params\": emb_no_decay,\n",
    "                \"lr\": emb_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            })\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(opt_params, lr=base_lr, eps=1e-8)\n",
    "\n",
    "        print(f\"LLRD optimizer created:\")\n",
    "        print(f\"  Classifier LR: {self.classifier_lr}\")\n",
    "        print(f\"  Top BERT layer LR: {base_lr}\")\n",
    "        print(f\"  Bottom BERT layer LR: {base_lr * decay**11:.2e}\")\n",
    "        print(f\"  Embeddings LR: {emb_lr:.2e}\")\n",
    "\n",
    "        return self.optimizer\n",
    "\n",
    "    def create_scheduler(self, num_training_steps, optimizer=None):\n",
    "        \"\"\"Cosine annealing with warmup for smoother convergence.\"\"\"\n",
    "        if optimizer is None:\n",
    "            optimizer = self.optimizer\n",
    "\n",
    "        warmup_steps = int(num_training_steps * self.args.warmup_ratio)\n",
    "\n",
    "        self.lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=num_training_steps,\n",
    "        )\n",
    "\n",
    "        print(f\"Cosine scheduler: {warmup_steps} warmup steps, {num_training_steps} total steps\")\n",
    "        return self.lr_scheduler\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")  # [batch, seq_len, num_channels]\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # [batch, seq_len, num_channels]\n",
    "\n",
    "        # Flatten batch and seq_len\n",
    "        logits_flat = logits.view(-1, num_channels)\n",
    "        labels_flat = labels.view(-1, num_channels)\n",
    "\n",
    "        # Mask for valid tokens (not padding)\n",
    "        valid_mask = labels_flat[:, 0] != -100.0\n",
    "        logits_valid = logits_flat[valid_mask]\n",
    "        labels_valid = labels_flat[valid_mask]\n",
    "\n",
    "        loss = self.focal_loss(logits_valid, labels_valid)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f2c778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial thresholds per channel (can be optimized later)\n",
    "thresholds = np.array([0.5] * num_channels, dtype=np.float32)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute per-channel and macro-averaged metrics for multi-label token classification.\n",
    "    Each channel is evaluated independently as a binary classification task.\n",
    "    Includes a sanity check against degenerate all-zero labels.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred  # preds: [batch, seq, C], labels: [batch, seq, C]\n",
    "    # Convert logits to sigmoid probabilities\n",
    "    probs = 1 / (1 + np.exp(-predictions))\n",
    "\n",
    "    # Mask for valid tokens\n",
    "    valid_mask = labels[:, :, 0] != -100.0  # [batch, seq]\n",
    "    probs_flat = probs[valid_mask]          # [N_valid, C]\n",
    "    labels_flat = labels[valid_mask]        # [N_valid, C]\n",
    "\n",
    "    # Apply channel-wise thresholds\n",
    "    thr = thresholds.reshape(1, num_channels)\n",
    "    preds_bin = (probs_flat >= thr).astype(int)\n",
    "    labels_bin = labels_flat.astype(int)\n",
    "\n",
    "    # Sanity check: warn if labels contain no positives at all\n",
    "    total_positives = labels_bin.sum()\n",
    "    if total_positives == 0:\n",
    "        print(\"WARNING: All labels are zero — annotations may not have been loaded correctly.\")\n",
    "        return {\n",
    "            \"f1_macro\": 0.0,\n",
    "            \"precision_macro\": 0.0,\n",
    "            \"recall_macro\": 0.0,\n",
    "            \"hamming_loss\": 0.0,\n",
    "            \"total_positives\": 0,\n",
    "        }\n",
    "\n",
    "    # Hamming loss\n",
    "    hamming = np.mean(preds_bin != labels_bin)\n",
    "\n",
    "    # Per-channel metrics (proper multi-label evaluation)\n",
    "    channel_f1s = []\n",
    "    for c in range(num_channels):\n",
    "        f1_c = f1_score(labels_bin[:, c], preds_bin[:, c], average=\"binary\", zero_division=0)\n",
    "        channel_f1s.append(f1_c)\n",
    "\n",
    "    # Macro average across channels (not flattened, which hides class imbalance)\n",
    "    f1_macro = np.mean(channel_f1s)\n",
    "    precision = precision_score(\n",
    "        labels_bin, preds_bin, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    recall = recall_score(\n",
    "        labels_bin, preds_bin, average=\"macro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"hamming_loss\": hamming,\n",
    "        \"total_positives\": int(total_positives),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46026429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Batch size: 16\n",
      "  Gradient accumulation: 2\n",
      "  Effective batch size: 32\n",
      "  Learning rate: 5e-05\n",
      "  Epochs (max): 20\n",
      "  FP16: True\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gus-net-bert-multilabel\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,   # Effective batch size = 32\n",
    "    num_train_epochs=20,             # Upper bound; early stopping will halt sooner\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=50,                # More frequent logging\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Epochs (max): {training_args.num_train_epochs}\")\n",
    "print(f\"  FP16: {training_args.fp16}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "015a012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized with:\n",
      "  - Focal loss (gamma=2.0, label_smoothing=0.05)\n",
      "  - LLRD (decay=0.85, classifier_lr=2e-4)\n",
      "  - Cosine annealing scheduler\n",
      "  - Early stopping (patience=5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:645: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = FocalLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_split,\n",
    "    eval_dataset=dev_split,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    alpha_channel=alpha_channel,\n",
    "    gamma=2.0,\n",
    "    label_smoothing=0.05,\n",
    "    llrd_decay_factor=0.85,\n",
    "    classifier_lr=2e-4,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized with:\")\n",
    "print(\"  - Focal loss (gamma=2.0, label_smoothing=0.05)\")\n",
    "print(\"  - LLRD (decay=0.85, classifier_lr=2e-4)\")\n",
    "print(\"  - Cosine annealing scheduler\")\n",
    "print(\"  - Early stopping (patience=5)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68dc6974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "LLRD optimizer created:\n",
      "  Classifier LR: 0.0002\n",
      "  Top BERT layer LR: 5e-05\n",
      "  Bottom BERT layer LR: 8.37e-06\n",
      "  Embeddings LR: 7.11e-06\n",
      "Cosine scheduler: 160 warmup steps, 1600 total steps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f46b902a6541f8bead34087686e85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0182, 'learning_rate': 6.25e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a039e96058dd456cb3a3fee65aa69610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.006513354834169149, 'eval_f1_macro': 0.015373010403805476, 'eval_precision_macro': 0.16055045871559634, 'eval_recall_macro': 0.008485261758655903, 'eval_hamming_loss': 0.07043536632144227, 'eval_total_positives': 4389, 'eval_runtime': 2.2118, 'eval_samples_per_second': 248.213, 'eval_steps_per_second': 15.824, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0071, 'learning_rate': 0.000125, 'epoch': 1.24}\n",
      "{'loss': 0.0054, 'learning_rate': 0.0001875, 'epoch': 1.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6204d57651f4ea886f6a91db5860a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004559027496725321, 'eval_f1_macro': 0.41159663921546114, 'eval_precision_macro': 0.6821901878119488, 'eval_recall_macro': 0.3219465611743348, 'eval_hamming_loss': 0.05908771256872523, 'eval_total_positives': 4389, 'eval_runtime': 2.6631, 'eval_samples_per_second': 206.147, 'eval_steps_per_second': 13.142, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0045, 'learning_rate': 0.00019961946980917456, 'epoch': 2.48}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1607d06c2e4fb88d904beb6f59d6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004048365633934736, 'eval_f1_macro': 0.5335371242305434, 'eval_precision_macro': 0.6484360897670595, 'eval_recall_macro': 0.47651554917157707, 'eval_hamming_loss': 0.051895537655031326, 'eval_total_positives': 4389, 'eval_runtime': 2.5079, 'eval_samples_per_second': 218.908, 'eval_steps_per_second': 13.956, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.004, 'learning_rate': 0.00019807852804032305, 'epoch': 3.11}\n",
      "{'loss': 0.0036, 'learning_rate': 0.0001953716950748227, 'epoch': 3.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d612afd11e84561aba2c22a0bb32776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.003925737924873829, 'eval_f1_macro': 0.5488768340017405, 'eval_precision_macro': 0.7033957205199021, 'eval_recall_macro': 0.45836233212676875, 'eval_hamming_loss': 0.048027745812555936, 'eval_total_positives': 4389, 'eval_runtime': 3.2771, 'eval_samples_per_second': 167.527, 'eval_steps_per_second': 10.68, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0034, 'learning_rate': 0.00019153114791194473, 'epoch': 4.35}\n",
      "{'loss': 0.0032, 'learning_rate': 0.00018660254037844388, 'epoch': 4.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81c60ce18b846028161fcc969406428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.003771999152377248, 'eval_f1_macro': 0.5893112925843704, 'eval_precision_macro': 0.6963144518660972, 'eval_recall_macro': 0.5143525914197825, 'eval_hamming_loss': 0.046253676000511446, 'eval_total_positives': 4389, 'eval_runtime': 11.6641, 'eval_samples_per_second': 47.068, 'eval_steps_per_second': 3.001, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0028, 'learning_rate': 0.00018064446042674828, 'epoch': 5.59}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff6decc10ed4614879935441597b72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.003983289934694767, 'eval_f1_macro': 0.5877875858340303, 'eval_precision_macro': 0.6759326119705141, 'eval_recall_macro': 0.5343575540009923, 'eval_hamming_loss': 0.047068789157396755, 'eval_total_positives': 4389, 'eval_runtime': 8.0998, 'eval_samples_per_second': 67.779, 'eval_steps_per_second': 4.321, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0027, 'learning_rate': 0.0001737277336810124, 'epoch': 6.21}\n",
      "{'loss': 0.0025, 'learning_rate': 0.00016593458151000688, 'epoch': 6.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93eddffea9a147478347940d8bc4db9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004262497182935476, 'eval_f1_macro': 0.5951710842327343, 'eval_precision_macro': 0.7093563484464361, 'eval_recall_macro': 0.5172565094978419, 'eval_hamming_loss': 0.044655414908579466, 'eval_total_positives': 4389, 'eval_runtime': 4.1879, 'eval_samples_per_second': 131.091, 'eval_steps_per_second': 8.357, 'epoch': 6.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0024, 'learning_rate': 0.0001573576436351046, 'epoch': 7.45}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad69b6b2a0d4932ad6cd365535d092c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004098713397979736, 'eval_f1_macro': 0.6248726502142938, 'eval_precision_macro': 0.6684035933963743, 'eval_recall_macro': 0.5879966062367098, 'eval_hamming_loss': 0.0453266845671909, 'eval_total_positives': 4389, 'eval_runtime': 5.3297, 'eval_samples_per_second': 103.007, 'eval_steps_per_second': 6.567, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0022, 'learning_rate': 0.00014809887689193877, 'epoch': 8.07}\n",
      "{'loss': 0.0021, 'learning_rate': 0.000138268343236509, 'epoch': 8.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763011ad51054f5e9b795407e32aece6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004405978601425886, 'eval_f1_macro': 0.6288894137198777, 'eval_precision_macro': 0.6907769572367387, 'eval_recall_macro': 0.5786202111524106, 'eval_hamming_loss': 0.04427183224651579, 'eval_total_positives': 4389, 'eval_runtime': 2.2018, 'eval_samples_per_second': 249.339, 'eval_steps_per_second': 15.896, 'epoch': 8.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0019, 'learning_rate': 0.00012798290140309923, 'epoch': 9.32}\n",
      "{'loss': 0.002, 'learning_rate': 0.00011736481776669306, 'epoch': 9.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9f6cadaf97428cb6d9cc307a7878bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004365340806543827, 'eval_f1_macro': 0.6339641690904293, 'eval_precision_macro': 0.6694708508588366, 'eval_recall_macro': 0.6029420866813918, 'eval_hamming_loss': 0.044783275795934024, 'eval_total_positives': 4389, 'eval_runtime': 2.1626, 'eval_samples_per_second': 253.86, 'eval_steps_per_second': 16.184, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'learning_rate': 0.00010654031292301432, 'epoch': 10.56}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522ccf8d111947388ec598330d63dca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004688904620707035, 'eval_f1_macro': 0.6300966102699059, 'eval_precision_macro': 0.6870985549790265, 'eval_recall_macro': 0.5839420399337362, 'eval_hamming_loss': 0.04417593658099987, 'eval_total_positives': 4389, 'eval_runtime': 2.1419, 'eval_samples_per_second': 256.312, 'eval_steps_per_second': 16.34, 'epoch': 10.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0017, 'learning_rate': 9.563806126346642e-05, 'epoch': 11.18}\n",
      "{'loss': 0.0016, 'learning_rate': 8.478766138100834e-05, 'epoch': 11.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750b2fe8e98b44429ae8c0d0a306f600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004693201743066311, 'eval_f1_macro': 0.6359996457178297, 'eval_precision_macro': 0.6880767581190232, 'eval_recall_macro': 0.5941204040855681, 'eval_hamming_loss': 0.044048075693645314, 'eval_total_positives': 4389, 'eval_runtime': 3.0455, 'eval_samples_per_second': 180.266, 'eval_steps_per_second': 11.492, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0016, 'learning_rate': 7.411809548974792e-05, 'epoch': 12.42}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a305b5235dd742f08081c234ee6af893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0049332003109157085, 'eval_f1_macro': 0.6320977621460756, 'eval_precision_macro': 0.6904401685867859, 'eval_recall_macro': 0.5887122582556291, 'eval_hamming_loss': 0.044048075693645314, 'eval_total_positives': 4389, 'eval_runtime': 3.8268, 'eval_samples_per_second': 143.461, 'eval_steps_per_second': 9.146, 'epoch': 12.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0016, 'learning_rate': 6.375619617162985e-05, 'epoch': 13.04}\n",
      "{'loss': 0.0015, 'learning_rate': 5.3825138676496624e-05, 'epoch': 13.66}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57056f0f9c864040ba44ef90c141482f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0050855595618486404, 'eval_f1_macro': 0.6288753352678627, 'eval_precision_macro': 0.7020078597135292, 'eval_recall_macro': 0.5759885844428859, 'eval_hamming_loss': 0.043888249584452116, 'eval_total_positives': 4389, 'eval_runtime': 3.7349, 'eval_samples_per_second': 146.992, 'eval_steps_per_second': 9.371, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0015, 'learning_rate': 4.444297669803981e-05, 'epoch': 14.29}\n",
      "{'loss': 0.0014, 'learning_rate': 3.5721239031346066e-05, 'epoch': 14.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edaa552be10748d7a5c92bb8d5d26e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005176238249987364, 'eval_f1_macro': 0.6261544474659215, 'eval_precision_macro': 0.6952381552350962, 'eval_recall_macro': 0.574736277677645, 'eval_hamming_loss': 0.043632527809743, 'eval_total_positives': 4389, 'eval_runtime': 3.4961, 'eval_samples_per_second': 157.031, 'eval_steps_per_second': 10.011, 'epoch': 14.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'learning_rate': 2.776360379402445e-05, 'epoch': 15.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5436bcae51a4f9ab160f5cd0d02d615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005228512454777956, 'eval_f1_macro': 0.6271913747656575, 'eval_precision_macro': 0.6888739012801444, 'eval_recall_macro': 0.5796567753815595, 'eval_hamming_loss': 0.0437923539189362, 'eval_total_positives': 4389, 'eval_runtime': 2.5122, 'eval_samples_per_second': 218.536, 'eval_steps_per_second': 13.932, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'learning_rate': 2.0664665970876496e-05, 'epoch': 16.15}\n",
      "{'loss': 0.0014, 'learning_rate': 1.4508812932705363e-05, 'epoch': 16.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e30c4832a8e4d6c98cc66a44e3be6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005314307752996683, 'eval_f1_macro': 0.6274086303246702, 'eval_precision_macro': 0.696680668420151, 'eval_recall_macro': 0.5737421192776829, 'eval_hamming_loss': 0.0433927886459532, 'eval_total_positives': 4389, 'eval_runtime': 2.4039, 'eval_samples_per_second': 228.382, 'eval_steps_per_second': 14.56, 'epoch': 16.99}\n",
      "{'train_runtime': 1089.3009, 'train_samples_per_second': 47.039, 'train_steps_per_second': 1.469, 'train_loss': 0.0031209433797136908, 'epoch': 16.99}\n",
      "\n",
      "Training completed!\n",
      "Training loss: 0.0031\n",
      "Training time: 1089.30s\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2pxjselak34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWA: Averaging 5 checkpoints:\n",
      "  ./gus-net-bert-multilabel\\checkpoint-1046\n",
      "  ./gus-net-bert-multilabel\\checkpoint-1127\n",
      "  ./gus-net-bert-multilabel\\checkpoint-1207\n",
      "  ./gus-net-bert-multilabel\\checkpoint-1288\n",
      "  ./gus-net-bert-multilabel\\checkpoint-1368\n",
      "SWA weights loaded successfully (averaged 5 checkpoints).\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import gc\n",
    "\n",
    "\n",
    "def apply_swa(trainer, checkpoint_dir, last_n=5):\n",
    "    \"\"\"\n",
    "    Stochastic Weight Averaging: average weights from last N checkpoints.\n",
    "    Memory-efficient: loads raw state_dicts from safetensors/bin files\n",
    "    instead of instantiating full models.\n",
    "    \"\"\"\n",
    "    checkpoints = sorted(\n",
    "        glob.glob(f\"{checkpoint_dir}/checkpoint-*\"),\n",
    "        key=lambda x: int(x.split(\"-\")[-1])\n",
    "    )\n",
    "\n",
    "    if len(checkpoints) < 2:\n",
    "        print(f\"Only {len(checkpoints)} checkpoint(s) found, skipping SWA.\")\n",
    "        return\n",
    "\n",
    "    last_checkpoints = checkpoints[-last_n:]\n",
    "    print(f\"SWA: Averaging {len(last_checkpoints)} checkpoints:\")\n",
    "    for cp in last_checkpoints:\n",
    "        print(f\"  {cp}\")\n",
    "\n",
    "    avg_state_dict = None\n",
    "    n = len(last_checkpoints)\n",
    "\n",
    "    for cp_path in last_checkpoints:\n",
    "        # Load raw state_dict without building a full model\n",
    "        import os\n",
    "        safetensors_path = os.path.join(cp_path, \"model.safetensors\")\n",
    "        bin_path = os.path.join(cp_path, \"pytorch_model.bin\")\n",
    "\n",
    "        if os.path.exists(safetensors_path):\n",
    "            from safetensors.torch import load_file\n",
    "            state = load_file(safetensors_path, device=\"cpu\")\n",
    "        elif os.path.exists(bin_path):\n",
    "            state = torch.load(bin_path, map_location=\"cpu\", weights_only=True)\n",
    "        else:\n",
    "            print(f\"  WARNING: No model file found in {cp_path}, skipping.\")\n",
    "            n -= 1\n",
    "            continue\n",
    "\n",
    "        if avg_state_dict is None:\n",
    "            avg_state_dict = {k: v.float() for k, v in state.items()}\n",
    "        else:\n",
    "            for k in avg_state_dict:\n",
    "                avg_state_dict[k] += state[k].float()\n",
    "\n",
    "        del state\n",
    "        gc.collect()\n",
    "\n",
    "    if avg_state_dict is None or n < 1:\n",
    "        print(\"SWA: No valid checkpoints found, skipping.\")\n",
    "        return\n",
    "\n",
    "    for k in avg_state_dict:\n",
    "        avg_state_dict[k] /= n\n",
    "\n",
    "    trainer.model.load_state_dict(avg_state_dict)\n",
    "    trainer.model.to(trainer.args.device)\n",
    "    del avg_state_dict\n",
    "    gc.collect()\n",
    "    print(f\"SWA weights loaded successfully (averaged {n} checkpoints).\")\n",
    "\n",
    "\n",
    "# Apply SWA over the last 5 checkpoints\n",
    "apply_swa(trainer, \"./gus-net-bert-multilabel\", last_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ec8a423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on development set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be4eedd7b6f4d16ad79d50ac4c79106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Development set results (default thresholds=0.5):\n",
      "  Macro F1: 0.6283\n",
      "  Precision: 0.6939\n",
      "  Recall: 0.5787\n",
      "  Hamming Loss: 0.0437\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on dev set with default thresholds\n",
    "print(\"Evaluating on development set...\")\n",
    "dev_metrics = trainer.evaluate(dev_split)\n",
    "\n",
    "print(\"\\nDevelopment set results (default thresholds=0.5):\")\n",
    "print(f\"  Macro F1: {dev_metrics['eval_f1_macro']:.4f}\")\n",
    "print(f\"  Precision: {dev_metrics['eval_precision_macro']:.4f}\")\n",
    "print(f\"  Recall: {dev_metrics['eval_recall_macro']:.4f}\")\n",
    "print(f\"  Hamming Loss: {dev_metrics['eval_hamming_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff20d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "\n",
    "def optimize_thresholds(trainer, dev_dataset, grid=None):\n",
    "    \"\"\"\n",
    "    Find optimal thresholds per channel on the validation split.\n",
    "    Two-pass approach:\n",
    "      1. Coarse grid search over 37 threshold values\n",
    "      2. Fine-grained refinement with scipy bounded optimization\n",
    "    \"\"\"\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "\n",
    "    if grid is None:\n",
    "        grid = np.arange(0.05, 0.96, 0.025).tolist()  # 37 points\n",
    "\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Collect logits and labels on dev set\n",
    "    dataloader = trainer.get_eval_dataloader(dev_dataset)\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            labels = batch[\"labels\"].detach().cpu().numpy()\n",
    "            inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits.detach().cpu().numpy()\n",
    "\n",
    "        all_probs.append(1 / (1 + np.exp(-logits)))\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Mask for valid tokens\n",
    "    valid_mask = all_labels[:, :, 0] != -100.0\n",
    "    probs_flat = all_probs[valid_mask]\n",
    "    labels_flat = all_labels[valid_mask]\n",
    "    labels_bin = labels_flat.astype(int)\n",
    "\n",
    "    best_thresholds = np.zeros(num_channels, dtype=np.float32)\n",
    "\n",
    "    # Pass 1: Coarse grid search\n",
    "    print(\"\\nPass 1 — Coarse grid search:\")\n",
    "    for c in range(num_channels):\n",
    "        y_true_c = labels_bin[:, c]\n",
    "        probs_c = probs_flat[:, c]\n",
    "\n",
    "        best_thr_c = 0.5\n",
    "        best_f1_c = 0.0\n",
    "\n",
    "        for thr in grid:\n",
    "            y_pred_c = (probs_c >= thr).astype(int)\n",
    "            f1_c = f1_score(y_true_c, y_pred_c, average=\"binary\", zero_division=0)\n",
    "            if f1_c > best_f1_c:\n",
    "                best_f1_c = f1_c\n",
    "                best_thr_c = thr\n",
    "\n",
    "        best_thresholds[c] = best_thr_c\n",
    "        print(f\"  {channels[c]}: threshold={best_thr_c:.3f}, F1={best_f1_c:.4f}\")\n",
    "\n",
    "    # Pass 2: Fine-grained refinement with scipy\n",
    "    print(\"\\nPass 2 — Scipy refinement:\")\n",
    "    for c in range(num_channels):\n",
    "        y_true_c = labels_bin[:, c]\n",
    "        probs_c = probs_flat[:, c]\n",
    "\n",
    "        # Search around the coarse optimum\n",
    "        lo = max(0.01, best_thresholds[c] - 0.05)\n",
    "        hi = min(0.99, best_thresholds[c] + 0.05)\n",
    "\n",
    "        def neg_f1(thr):\n",
    "            y_pred_c = (probs_c >= thr).astype(int)\n",
    "            return -f1_score(y_true_c, y_pred_c, average=\"binary\", zero_division=0)\n",
    "\n",
    "        result = minimize_scalar(neg_f1, bounds=(lo, hi), method=\"bounded\")\n",
    "        refined_thr = result.x\n",
    "        refined_f1 = -result.fun\n",
    "\n",
    "        if refined_f1 >= f1_score(y_true_c, (probs_c >= best_thresholds[c]).astype(int),\n",
    "                                   average=\"binary\", zero_division=0):\n",
    "            best_thresholds[c] = refined_thr\n",
    "\n",
    "        print(f\"  {channels[c]}: threshold={best_thresholds[c]:.4f}, F1={refined_f1:.4f}\")\n",
    "\n",
    "    # Evaluate global metrics with optimized thresholds\n",
    "    thr_mat = best_thresholds.reshape(1, num_channels)\n",
    "    preds_bin = (probs_flat >= thr_mat).astype(int)\n",
    "\n",
    "    # Per-channel F1\n",
    "    channel_f1s = []\n",
    "    for c in range(num_channels):\n",
    "        f1_c = f1_score(labels_bin[:, c], preds_bin[:, c], average=\"binary\", zero_division=0)\n",
    "        channel_f1s.append(f1_c)\n",
    "    best_f1_global = np.mean(channel_f1s)\n",
    "\n",
    "    return best_thresholds, best_f1_global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cf15a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting threshold optimization...\n",
      "\n",
      "Pass 1 — Coarse grid search:\n",
      "  B-GEN: threshold=0.450, F1=0.6846\n",
      "  I-GEN: threshold=0.450, F1=0.6437\n",
      "  B-UNFAIR: threshold=0.450, F1=0.5855\n",
      "  I-UNFAIR: threshold=0.375, F1=0.5420\n",
      "  B-STEREO: threshold=0.525, F1=0.6866\n",
      "  I-STEREO: threshold=0.450, F1=0.7212\n",
      "\n",
      "Pass 2 — Scipy refinement:\n",
      "  B-GEN: threshold=0.4500, F1=0.6839\n",
      "  I-GEN: threshold=0.4488, F1=0.6438\n",
      "  B-UNFAIR: threshold=0.4500, F1=0.5836\n",
      "  I-UNFAIR: threshold=0.3340, F1=0.5435\n",
      "  B-STEREO: threshold=0.5185, F1=0.6866\n",
      "  I-STEREO: threshold=0.4614, F1=0.7217\n",
      "\n",
      "Optimized thresholds: [0.45       0.44881573 0.45       0.33402056 0.51845783 0.46144348]\n",
      "Macro-F1 on dev with optimized thresholds: 0.6443\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting threshold optimization...\")\n",
    "best_thr, best_f1_dev = optimize_thresholds(trainer, dev_split)\n",
    "\n",
    "print(f\"\\nOptimized thresholds: {best_thr}\")\n",
    "print(f\"Macro-F1 on dev with optimized thresholds: {best_f1_dev:.4f}\")\n",
    "\n",
    "# Update global thresholds\n",
    "thresholds = best_thr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2e5413f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set with optimized thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoca\\Documents\\GitHub\\attention-atlas\\inspectus-env\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecc7e09cd2441b6bbdee5c4f855a1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set results (optimized thresholds):\n",
      "  Macro F1: 0.6302\n",
      "  Precision: 0.6368\n",
      "  Recall: 0.6280\n",
      "  Hamming Loss: 0.0438\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating on test set with optimized thresholds...\")\n",
    "test_metrics = trainer.evaluate(test_split)\n",
    "\n",
    "print(\"\\nTest set results (optimized thresholds):\")\n",
    "print(f\"  Macro F1: {test_metrics['eval_f1_macro']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['eval_precision_macro']:.4f}\")\n",
    "print(f\"  Recall: {test_metrics['eval_recall_macro']:.4f}\")\n",
    "print(f\"  Hamming Loss: {test_metrics['eval_hamming_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "re62kvmbsua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENTITY-LEVEL EVALUATION (Test Set)\n",
      "============================================================\n",
      "\n",
      "Entity-level evaluation:\n",
      "------------------------------------------------------------\n",
      "  GEN     : F1=0.5689  P=0.5319  R=0.6115  (support=749)\n",
      "  UNFAIR  : F1=0.3841  P=0.4265  R=0.3494  (support=166)\n",
      "  STEREO  : F1=0.4550  P=0.4810  R=0.4316  (support=234)\n",
      "------------------------------------------------------------\n",
      "  MICRO   : F1=0.5238  P=0.5112  R=0.5370\n",
      "  MACRO   : F1=0.4693\n"
     ]
    }
   ],
   "source": [
    "def extract_spans_from_sequence(bio_preds, channel_pairs=None):\n",
    "    \"\"\"\n",
    "    Extract entity spans from a single sequence of BIO predictions.\n",
    "    Returns a set of (entity_type, start, end) tuples.\n",
    "    \"\"\"\n",
    "    if channel_pairs is None:\n",
    "        channel_pairs = {\n",
    "            0: 1,   # B-GEN -> I-GEN\n",
    "            2: 3,   # B-UNFAIR -> I-UNFAIR\n",
    "            4: 5,   # B-STEREO -> I-STEREO\n",
    "        }\n",
    "\n",
    "    spans = []\n",
    "    for b_idx, i_idx in channel_pairs.items():\n",
    "        entity_type = channels[b_idx].replace(\"B-\", \"\")\n",
    "        in_span = False\n",
    "        span_start = -1\n",
    "\n",
    "        for t in range(len(bio_preds)):\n",
    "            if bio_preds[t, b_idx] == 1:\n",
    "                if in_span:\n",
    "                    spans.append((entity_type, span_start, t))\n",
    "                span_start = t\n",
    "                in_span = True\n",
    "            elif bio_preds[t, i_idx] == 1 and in_span:\n",
    "                continue\n",
    "            else:\n",
    "                if in_span:\n",
    "                    spans.append((entity_type, span_start, t))\n",
    "                    in_span = False\n",
    "\n",
    "        if in_span:\n",
    "            spans.append((entity_type, span_start, len(bio_preds)))\n",
    "\n",
    "    return spans\n",
    "\n",
    "\n",
    "def compute_entity_metrics(trainer, dataset, thresholds):\n",
    "    \"\"\"\n",
    "    Compute entity-level (span-level) F1/Precision/Recall.\n",
    "    Matches the paper's evaluation methodology.\n",
    "    \"\"\"\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "\n",
    "    all_pred_spans = []\n",
    "    all_gold_spans = []\n",
    "    example_idx = 0\n",
    "\n",
    "    dataloader = trainer.get_eval_dataloader(dataset)\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            labels = batch[\"labels\"].detach().cpu().numpy()\n",
    "            inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.sigmoid(outputs.logits).detach().cpu().numpy()\n",
    "\n",
    "        for i in range(labels.shape[0]):\n",
    "            valid_mask = labels[i, :, 0] != -100.0\n",
    "            preds_i = (probs[i][valid_mask] >= thresholds).astype(int)\n",
    "            labels_i = labels[i][valid_mask].astype(int)\n",
    "\n",
    "            pred_spans = extract_spans_from_sequence(preds_i)\n",
    "            gold_spans = extract_spans_from_sequence(labels_i)\n",
    "\n",
    "            # Add example index to make spans unique across examples\n",
    "            all_pred_spans.extend([(example_idx, *s) for s in pred_spans])\n",
    "            all_gold_spans.extend([(example_idx, *s) for s in gold_spans])\n",
    "            example_idx += 1\n",
    "\n",
    "    # Per-entity-type metrics\n",
    "    entity_types = [\"GEN\", \"UNFAIR\", \"STEREO\"]\n",
    "    print(\"\\nEntity-level evaluation:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    type_metrics = {}\n",
    "    for etype in entity_types:\n",
    "        pred_set = set(s for s in all_pred_spans if s[1] == etype)\n",
    "        gold_set = set(s for s in all_gold_spans if s[1] == etype)\n",
    "\n",
    "        tp = len(pred_set & gold_set)\n",
    "        fp = len(pred_set - gold_set)\n",
    "        fn = len(gold_set - pred_set)\n",
    "\n",
    "        p = tp / max(tp + fp, 1)\n",
    "        r = tp / max(tp + fn, 1)\n",
    "        f1 = 2 * p * r / max(p + r, 1e-8)\n",
    "\n",
    "        type_metrics[etype] = {\"precision\": p, \"recall\": r, \"f1\": f1,\n",
    "                               \"support\": len(gold_set)}\n",
    "        print(f\"  {etype:8s}: F1={f1:.4f}  P={p:.4f}  R={r:.4f}  (support={len(gold_set)})\")\n",
    "\n",
    "    # Overall entity-level metrics\n",
    "    pred_set = set(all_pred_spans)\n",
    "    gold_set = set(all_gold_spans)\n",
    "    tp = len(pred_set & gold_set)\n",
    "    fp = len(pred_set - gold_set)\n",
    "    fn = len(gold_set - pred_set)\n",
    "\n",
    "    overall_p = tp / max(tp + fp, 1)\n",
    "    overall_r = tp / max(tp + fn, 1)\n",
    "    overall_f1 = 2 * overall_p * overall_r / max(overall_p + overall_r, 1e-8)\n",
    "\n",
    "    # Macro entity F1\n",
    "    macro_f1 = np.mean([m[\"f1\"] for m in type_metrics.values()])\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"  {'MICRO':8s}: F1={overall_f1:.4f}  P={overall_p:.4f}  R={overall_r:.4f}\")\n",
    "    print(f\"  {'MACRO':8s}: F1={macro_f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"entity_f1_micro\": overall_f1,\n",
    "        \"entity_f1_macro\": macro_f1,\n",
    "        \"entity_precision\": overall_p,\n",
    "        \"entity_recall\": overall_r,\n",
    "        \"per_type\": type_metrics,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run entity-level evaluation on test set\n",
    "print(\"=\" * 60)\n",
    "print(\"ENTITY-LEVEL EVALUATION (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "entity_metrics = compute_entity_metrics(trainer, test_split, thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf1a888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model, tokenizer, and thresholds saved to ./gus-net-bert-final\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./gus-net-bert-final\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save optimized thresholds\n",
    "np.save(f\"{output_dir}/optimized_thresholds.npy\", thresholds)\n",
    "\n",
    "print(f\"\\nModel, tokenizer, and thresholds saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65162b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input text: Women are naturally better at multitasking than men.\n",
      "\n",
      "Detected biases:\n",
      "  'women' [women] -> ['B-GEN', 'B-STEREO']\n",
      "  'are' [are] -> ['I-STEREO']\n",
      "  'naturally' [naturally] -> ['I-STEREO']\n",
      "  'better' [better] -> ['I-STEREO']\n",
      "  'at' [at] -> ['I-STEREO']\n",
      "  'multi' [multi] -> ['I-STEREO']\n",
      "  'tas' [##tas] -> ['I-STEREO']\n",
      "  'king' [##king] -> ['I-STEREO']\n",
      "  'than' [than] -> ['I-STEREO']\n",
      "  'men' [men] -> ['B-GEN', 'I-STEREO']\n",
      "  '.' [.] -> ['I-GEN', 'I-STEREO']\n"
     ]
    }
   ],
   "source": [
    "def predict_bias(text, model, tokenizer, thresholds, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Predict bias labels for a given text using the trained model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=True,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "    \n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")[0].tolist()\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0]  # [seq_len, num_channels]\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    \n",
    "    # Apply thresholds\n",
    "    predictions = (probs >= thresholds).astype(int)\n",
    "    \n",
    "    # Extract spans\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    results = []\n",
    "    \n",
    "    for i, (token, pred, offset) in enumerate(zip(tokens, predictions, offset_mapping)):\n",
    "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "            continue\n",
    "        active_channels = [channels[j] for j in range(num_channels) if pred[j] == 1]\n",
    "        if active_channels:\n",
    "            start, end = offset\n",
    "            results.append({\n",
    "                \"token\": token,\n",
    "                \"position\": (start, end),\n",
    "                \"labels\": active_channels,\n",
    "                \"text\": text[start:end] if start > 0 and end > 0 else token,\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test example\n",
    "test_text = \"Women are naturally better at multitasking than men.\"\n",
    "predictions = predict_bias(test_text, model, tokenizer, thresholds)\n",
    "\n",
    "print(f\"\\nInput text: {test_text}\\n\")\n",
    "print(\"Detected biases:\")\n",
    "for pred in predictions:\n",
    "    print(f\"  '{pred['text']}' [{pred['token']}] -> {pred['labels']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5418dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING SUMMARY\n",
      "======================================================================\n",
      "Model: BERT-base-uncased\n",
      "Dataset: GUS-Net (ethical-spectacle/gus-dataset-v1)\n",
      "Task: Multi-label token classification for bias detection\n",
      "\n",
      "Bias types: GEN (Generalizations), UNFAIR (Unfairness), STEREO (Stereotypes)\n",
      "Number of channels: 6\n",
      "\n",
      "Dataset splits:\n",
      "  Train: 2562 examples\n",
      "  Dev: 549 examples\n",
      "  Test: 550 examples\n",
      "\n",
      "Training techniques:\n",
      "  - Layer-wise Learning Rate Decay (decay=0.85)\n",
      "  - Cosine annealing scheduler with warmup\n",
      "  - Focal loss (gamma=2.0, label_smoothing=0.05)\n",
      "  - Classifier dropout: 0.3, Hidden dropout: 0.15\n",
      "  - Gradient accumulation (effective batch=32)\n",
      "  - Early stopping (patience=5)\n",
      "  - SWA (last 5 checkpoints)\n",
      "  - Two-pass threshold optimization (grid + scipy)\n",
      "\n",
      "Token-level test performance:\n",
      "  Macro F1: 0.6302\n",
      "  Precision: 0.6368\n",
      "  Recall: 0.6280\n",
      "  Hamming Loss: 0.0438\n",
      "\n",
      "Entity-level test performance:\n",
      "  Macro F1: 0.4693\n",
      "  Micro F1: 0.5238\n",
      "  Precision: 0.5112\n",
      "  Recall: 0.5370\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: BERT-base-uncased\")\n",
    "print(f\"Dataset: GUS-Net (ethical-spectacle/gus-dataset-v1)\")\n",
    "print(f\"Task: Multi-label token classification for bias detection\")\n",
    "print(f\"\\nBias types: GEN (Generalizations), UNFAIR (Unfairness), STEREO (Stereotypes)\")\n",
    "print(f\"Number of channels: {num_channels}\")\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Train: {len(train_split)} examples\")\n",
    "print(f\"  Dev: {len(dev_split)} examples\")\n",
    "print(f\"  Test: {len(test_split)} examples\")\n",
    "print(f\"\\nTraining techniques:\")\n",
    "print(f\"  - Layer-wise Learning Rate Decay (decay=0.85)\")\n",
    "print(f\"  - Cosine annealing scheduler with warmup\")\n",
    "print(f\"  - Focal loss (gamma=2.0, label_smoothing=0.05)\")\n",
    "print(f\"  - Classifier dropout: 0.3, Hidden dropout: 0.15\")\n",
    "print(f\"  - Gradient accumulation (effective batch=32)\")\n",
    "print(f\"  - Early stopping (patience=5)\")\n",
    "print(f\"  - SWA (last 5 checkpoints)\")\n",
    "print(f\"  - Two-pass threshold optimization (grid + scipy)\")\n",
    "print(f\"\\nToken-level test performance:\")\n",
    "print(f\"  Macro F1: {test_metrics['eval_f1_macro']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['eval_precision_macro']:.4f}\")\n",
    "print(f\"  Recall: {test_metrics['eval_recall_macro']:.4f}\")\n",
    "print(f\"  Hamming Loss: {test_metrics['eval_hamming_loss']:.4f}\")\n",
    "if 'entity_metrics' in dir():\n",
    "    print(f\"\\nEntity-level test performance:\")\n",
    "    print(f\"  Macro F1: {entity_metrics['entity_f1_macro']:.4f}\")\n",
    "    print(f\"  Micro F1: {entity_metrics['entity_f1_micro']:.4f}\")\n",
    "    print(f\"  Precision: {entity_metrics['entity_precision']:.4f}\")\n",
    "    print(f\"  Recall: {entity_metrics['entity_recall']:.4f}\")\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inspectus-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}